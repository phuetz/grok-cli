/**
 * Provider Types
 *
 * Type definitions for the multi-LLM provider system.
 * This module defines the common interfaces used across all AI providers
 * to ensure a unified API for the rest of the application.
 */

/**
 * Supported AI provider identifiers.
 */
export type ProviderType = 'grok' | 'claude' | 'openai' | 'gemini' | 'ollama' | 'lm-studio';

/**
 * Capabilities that a provider may or may not support.
 * - `streaming`: Support for streaming responses.
 * - `tools`: Support for tool use/function calling.
 * - `vision`: Support for image/multimodal input.
 * - `json_mode`: Support for enforced JSON output.
 * - `function_calling`: Alias for tools (legacy).
 */
export type ProviderFeature = 'streaming' | 'tools' | 'vision' | 'json_mode' | 'function_calling';

/**
 * Standardized message format for LLM communication.
 */
export interface LLMMessage {
  /** The role of the message sender. */
  role: 'system' | 'user' | 'assistant' | 'tool';
  /** The content of the message. */
  content: string;
  /** The name of the author of this message. `name` is required if role is `function`. */
  name?: string;
  /** The tool call ID this message is responding to (required for role='tool'). */
  tool_call_id?: string;
  /** The tool calls generated by the model (required for role='assistant' if tools were used). */
  tool_calls?: ToolCall[];
}

/**
 * Represents a tool call request from an LLM.
 */
export interface ToolCall {
  /** Unique identifier for the tool call. */
  id: string;
  /** The type of tool call (currently only 'function' is supported). */
  type: 'function';
  /** The function call details. */
  function: {
    /** The name of the function/tool to call. */
    name: string;
    /** The arguments to pass to the function (JSON string). */
    arguments: string;
  };
}

/**
 * Definition of a tool available to the LLM.
 */
export interface ToolDefinition {
  /** The name of the tool. */
  name: string;
  /** A description of what the tool does. */
  description: string;
  /** JSON Schema defining the expected parameters. */
  parameters: Record<string, unknown>;
}

/**
 * Standardized response from an LLM provider.
 */
export interface LLMResponse {
  /** Unique identifier for the completion. */
  id: string;
  /** The textual content of the response (null if only tools are called). */
  content: string | null;
  /** Any tool calls generated by the model. */
  toolCalls: ToolCall[];
  /** The reason the generation stopped. */
  finishReason: 'stop' | 'tool_calls' | 'length' | 'content_filter' | null;
  /** Token usage statistics. */
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  /** The specific model version used. */
  model: string;
  /** The provider that generated the response. */
  provider: ProviderType;
}

/**
 * A chunk of a streaming response.
 */
export interface StreamChunk {
  /** The type of data this chunk contains. */
  type: 'content' | 'tool_call' | 'done' | 'error';
  /** Text content delta (present if type='content'). */
  content?: string;
  /** Partial tool call data (present if type='tool_call'). */
  toolCall?: Partial<ToolCall>;
  /** Error message (present if type='error'). */
  error?: string;
}

/**
 * Configuration options for initializing a provider.
 */
export interface ProviderConfig {
  /** The API key for authentication. */
  apiKey: string;
  /** The specific model identifier to use (optional, defaults to provider default). */
  model?: string;
  /** Custom base URL for API requests (e.g., for local proxies or enterprise endpoints). */
  baseUrl?: string;
  /** Maximum number of tokens to generate. */
  maxTokens?: number;
  /** Sampling temperature (0-1). Higher values are more random. */
  temperature?: number;
  /** Request timeout in milliseconds. */
  timeout?: number;
  /** Maximum number of retries for failed requests. */
  maxRetries?: number;
}

/**
 * Options for a chat completion request.
 */
export interface CompletionOptions {
  /** The conversation history. */
  messages: LLMMessage[];
  /** List of available tools (optional). */
  tools?: ToolDefinition[];
  /** Sampling temperature (override provider default). */
  temperature?: number;
  /** Maximum tokens to generate (override provider default). */
  maxTokens?: number;
  /** Whether to stream the response. */
  stream?: boolean;
  /** System prompt to prepend or insert into messages. */
  systemPrompt?: string;
  /** Force the model to use tools (mode ANY vs AUTO). */
  forceToolUse?: boolean;
  /** Current iteration in the tool calling loop (used to detect first call). */
  toolCallIteration?: number;
}

/** Internal interface for Anthropic API response */
export interface AnthropicResponse {
  id: string;
  model: string;
  content: Array<{ type: string; text?: string; id?: string; name?: string; input?: unknown }>;
  stop_reason: string | null;
  usage: { input_tokens: number; output_tokens: number };
}

/** Internal interface for Anthropic stream events */
export interface AnthropicStreamEvent {
  type: string;
  delta?: { type: string; text?: string; partial_json?: string };
}
