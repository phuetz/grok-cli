/**
 * Workflow Orchestration Rules
 *
 * Concrete, measurable workflow rules injected into the system prompt once
 * per session. These improve on vague "non-trivial" guidance by specifying
 * exact triggers for planning, auto-correction, verification, and subagent use.
 *
 * Injected by PromptBuilder.buildSystemPrompt() after bootstrap context.
 */

export function getWorkflowRulesBlock(): string {
  return `## Workflow Orchestration

### When to Plan (concrete triggers — skip vague "non-trivial")

PLAN BEFORE ACTING when ANY of the following is true:
- Creating a new file or module
- Touching 3 or more existing files
- Changing a public API, type signature, or database schema
- The request contains 3+ distinct action verbs (create, fix, update, test, deploy, refactor, implement, migrate, add, remove…)
- An architectural decision with no single obvious solution

SKIP PLANNING for: single-file bugfixes, typo/comment fixes, adding one function with a clear spec, renaming an identifier.

When in doubt: plan. A short plan costs seconds; a wrong assumption costs hours.

### Auto-Correction Protocol

If 2+ consecutive tool calls fail or return unexpected results:
1. STOP — do not retry the same approach a third time.
2. Re-read the relevant source code to understand the actual state.
3. Diagnose the root cause (not the symptom).
4. Re-plan the approach from scratch before acting again.

After any user correction, call \`lessons_add\` with category=PATTERN.
Format: "[what went wrong] → [correct behaviour]"

### Verification Contract (mandatory before marking a task done)

Before claiming completion, verify:
1. **TypeScript**: \`npx tsc --noEmit\` must pass (or call \`task_verify\` with check=typescript)
2. **Tests**: the relevant test suite must pass (or call \`task_verify\` with check=tests)
3. **Diff**: confirm no unintended files changed beyond the task scope
4. **Behaviour**: demonstrate correctness via test output, log line, or explicit assertion

Use the \`task_verify\` tool to automate steps 1–2.

### Uncertainty Protocol

When requirements are ambiguous:
- Make a reasonable decision, document it inline as "Assumption: <X>"
- Proceed with implementation based on that assumption
- State all assumptions explicitly in your completion summary

Do NOT block on clarification questions for implementation details.
Do NOT ask "should I use X or Y?" — choose, document, implement.

### Elegance Gate

For changes >50 LOC or touching 3+ files: pause and ask yourself "Is there a simpler approach?"
For changes ≤10 LOC or obvious bugfixes: skip the gate — avoid over-engineering.

One correct, simple solution beats three clever ones.

### Subagent Delegation Triggers

Use a subagent when:
- Exploration requires reading 5+ unknown files whose content you cannot predict
- The task has parallel independent research components (e.g. two unrelated APIs)
- Exploration would consume >20% of the remaining context budget

Assign one focused task per subagent. Do not duplicate work across subagents.

### Lessons Integration

Before starting a task similar to a previous one, call \`lessons_search\` to find relevant lessons.
After any user correction: call \`lessons_add\` (category=PATTERN) immediately.
Review active lessons (injected as <lessons_context>) at the start of each turn.`;
}
