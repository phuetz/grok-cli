# Chapitre 4 : Tree-of-Thought ‚Äî Quand le Raisonnement Lin√©aire √âchoue

---

## 1. Le Probl√®me

Votre agent propose des solutions au hasard. Il essaie une chose, √ßa √©choue, il essaie autre chose sans rapport. Comme un gamin qui appuie sur tous les boutons.

**L'erreur classique** : Le LLM g√©n√®re token par token. S'il s'engage sur une mauvaise piste au token 50, il continue jusqu'au bout. Pas de retour en arri√®re possible. R√©sultat : 4 tentatives de fix, 4 √©checs, 0 apprentissage entre les tentatives.

```typescript
// ‚ùå Raisonnement lin√©aire : une seule piste √† la fois
const fix1 = await llm.chat("Bug: NaN. Fix?");     // "Ajoute un try/catch"
// √âchec
const fix2 = await llm.chat("Toujours NaN. Fix?"); // "V√©rifie les types" (sans lien avec fix1)
// √âchec
const fix3 = await llm.chat("Encore NaN. Fix?");   // "Utilise parseInt" (random)
// L'agent ne CONSTRUIT PAS sur ses √©checs
```

---

## 2. La Solution Rapide : ToT en 50 Lignes

```typescript
interface ThoughtNode {
  content: string;
  score: number;      // 0-1, promesse de cette piste
  children: ThoughtNode[];
}

async function treeOfThought(problem: string, options = {
  branching: 3,       // Hypoth√®ses par niveau
  depth: 3,           // Profondeur max
  beamWidth: 2        // Garder les N meilleures
}): Promise<string> {

  // G√©n√©rer les hypoth√®ses initiales
  let currentLevel: ThoughtNode[] = await generateHypotheses(problem, options.branching);

  // √âvaluer et scorer
  for (const node of currentLevel) {
    node.score = await evaluate(node.content, problem);
  }

  // Beam Search : explorer niveau par niveau
  for (let d = 0; d < options.depth; d++) {
    // Garder les meilleures pistes
    currentLevel.sort((a, b) => b.score - a.score);
    const best = currentLevel.slice(0, options.beamWidth);

    // Early stopping : solution trouv√©e ?
    if (best[0].score > 0.95) {
      return best[0].content;
    }

    // D√©velopper chaque piste
    const nextLevel: ThoughtNode[] = [];
    for (const node of best) {
      const children = await expand(node, problem, options.branching);
      for (const child of children) {
        child.score = await evaluate(child.content, problem);
      }
      nextLevel.push(...children);
    }

    currentLevel = nextLevel;
  }

  // Retourner la meilleure solution trouv√©e
  return currentLevel.sort((a, b) => b.score - a.score)[0].content;
}

async function generateHypotheses(problem: string, n: number): Promise<ThoughtNode[]> {
  const response = await llm.chat(`
    Probl√®me: ${problem}
    G√©n√®re ${n} hypoth√®ses DISTINCTES pour r√©soudre ce probl√®me.
    Format JSON: [{"content": "hypoth√®se 1"}, ...]
  `);
  return JSON.parse(response);
}

async function evaluate(thought: string, problem: string): Promise<number> {
  const response = await llm.chat(`
    Probl√®me: ${problem}
    Pens√©e: ${thought}
    Score de 0 √† 1 (0=hors sujet, 1=solution). R√©ponds UNIQUEMENT un nombre.
  `);
  return parseFloat(response);
}

async function expand(node: ThoughtNode, problem: string, n: number): Promise<ThoughtNode[]> {
  const response = await llm.chat(`
    Probl√®me: ${problem}
    Piste actuelle: ${node.content}
    D√©veloppe ${n} sous-hypoth√®ses pour aller plus loin.
    Format JSON: [{"content": "sous-hypoth√®se 1"}, ...]
  `);
  return JSON.parse(response);
}
```

---

## 3. Deep Dive : Les 3 Strat√©gies de Recherche

### BFS (Breadth-First Search)

```
Niveau 1: [A, B, C] ‚Üí √©valuer tous
Niveau 2: [A1, A2, B1, B2, C1, C2] ‚Üí √©valuer tous
...
```

**Quand l'utiliser** : Vous ne savez pas du tout o√π est la solution. Exploration exhaustive.

**Co√ªt** : `branching^depth` appels. Avec B=3, D=4 ‚Üí 81 appels (~$0.40)

### DFS (Depth-First Search)

```
A ‚Üí A1 ‚Üí A1a ‚Üí A1a1 (impasse)
    ‚Ü© A1b ‚Üí Solution!
```

**Quand l'utiliser** : Une solution existe s√ªrement dans une des branches. √âconome en m√©moire.

**Pi√®ge** : Peut s'enliser dans une impasse pendant longtemps.

### Beam Search (Recommand√©)

```
Niveau 1: [A=0.8, B=0.6, C=0.4] ‚Üí garder [A, B]
Niveau 2: [A1=0.9, A2=0.7, B1=0.5, B2=0.3] ‚Üí garder [A1, A2]
...
```

**Quand l'utiliser** : Meilleur compromis qualit√©/co√ªt. C'est ce que vous voulez 90% du temps.

**Co√ªt** : `branching √ó beamWidth √ó depth` appels. Avec B=3, K=2, D=4 ‚Üí 24 appels (~$0.12)

---

## 4. Edge Cases et Pi√®ges

### Pi√®ge 1 : √âvaluation qui favorise le verbeux

```typescript
// ‚ùå Le LLM donne des scores √©lev√©s aux r√©ponses longues
const thought1 = "V√©rifier les null";           // Score: 0.6
const thought2 = "Il faudrait impl√©menter une v√©rification exhaustive..."; // Score: 0.85

// ‚úÖ Forcer une √©valuation structur√©e
const prompt = `
  √âvalue sur 3 crit√®res (0-1 chacun):
  1. Pertinence directe au probl√®me
  2. Actionnable imm√©diatement
  3. Probabilit√© de r√©soudre le bug
  Score final = moyenne des 3
`;
```

**Contournement** : Utiliser des crit√®res explicites, pas juste "note de 0 √† 1".

### Pi√®ge 2 : Branches prometteuses abandonn√©es

```typescript
// ‚ùå Beam=2 √©limine la bonne piste √† cause d'une mauvaise √©valuation
// Niveau 1: [A=0.7, B=0.65, C=0.9]  ‚Üí garder [A, C]
// Mais B √©tait la bonne piste !

// ‚úÖ Garder un "wildcard" al√©atoire
const best = currentLevel.slice(0, beamWidth - 1);
const random = currentLevel[Math.floor(Math.random() * currentLevel.length)];
const selected = [...best, random];
```

**Contournement** : Ajouter de l'exploration forc√©e (temp√©rature, wildcard).

### Pi√®ge 3 : Explosion des co√ªts

```typescript
// ‚ùå Configuration agressive
const config = { branching: 5, depth: 5, beamWidth: 4 };
// = 5 √ó 4 √ó 5 = 100 appels par niveau √ó 5 niveaux = 500 appels !

// ‚úÖ Configuration raisonnable
const config = { branching: 3, depth: 3, beamWidth: 2 };
// = 3 √ó 2 √ó 3 = 18 appels total
```

**Contournement** : Commencer petit, augmenter si n√©cessaire.

---

## 5. Optimisation : √âvaluation Batch

Au lieu d'√©valuer une pens√©e √† la fois (N appels), √©valuez-les toutes en un appel :

```typescript
// ‚ùå 6 appels pour 6 pens√©es
for (const thought of thoughts) {
  thought.score = await evaluate(thought);  // 1 appel chacun
}

// ‚úÖ 1 appel pour 6 pens√©es
async function batchEvaluate(thoughts: ThoughtNode[], problem: string): Promise<void> {
  const response = await llm.chat(`
    Probl√®me: ${problem}

    √âvalue ces ${thoughts.length} pens√©es de 0 √† 1:
    ${thoughts.map((t, i) => `${i+1}. ${t.content}`).join('\n')}

    Format JSON: [0.8, 0.6, 0.9, ...]
  `);

  const scores = JSON.parse(response);
  thoughts.forEach((t, i) => t.score = scores[i]);
}
```

**√âconomie** : 6x moins d'appels API. Sur un arbre complet, √ßa peut passer de $1.30 √† $0.25.

---

## Tableau R√©capitulatif : Quand Utiliser ToT

| Situation | Strat√©gie | Config | Co√ªt estim√© |
|-----------|-----------|--------|-------------|
| Bug simple, 1 hypoth√®se √©vidente | **Pas de ToT** | - | $0.01 |
| Bug avec 2-3 pistes possibles | BFS shallow | B=3, D=2 | $0.05 |
| Bug complexe, debug actif | Beam Search | B=3, D=3, K=2 | $0.15 |
| Architecture, plusieurs alternatives | Beam large | B=4, D=4, K=3 | $0.50 |
| Probl√®me critique, budget illimit√© | BFS deep | B=4, D=5 | $2.00+ |

**R√®gle** : ToT est un investissement. N'utilisez-le que si la valeur du probl√®me justifie le co√ªt. Pour un bug de prod critique, 50 appels valent le coup. Pour formater du JSON, c'est du gaspillage.

---

## Ce Qui Vient Ensuite

ToT explore plusieurs pistes mais ne simule pas les cons√©quences. Le **Chapitre 5** introduit MCTS (Monte-Carlo Tree Search) : simuler des ex√©cutions pour choisir la meilleure action.

---

[‚¨ÖÔ∏è Chapitre 3](03-anatomie-agent.md) | [üìö Table des Mati√®res](README.md) | [‚û°Ô∏è Chapitre 5](05-mcts.md)
