# Chapitre 9 â€” Context Compression & Masking ğŸ—œï¸

---

## ğŸ¬ ScÃ¨ne d'ouverture

*3h47 du matin. Le tÃ©lÃ©phone de Lina vibre. Un email de son service cloud : "Alerte budget : 90% de votre limite mensuelle atteinte."*

*Elle s'assoit dans son lit, le cÅ“ur battant. On n'est que le 12 du mois.*

*Le lendemain matin, elle ouvre sa facture API avec une boule au ventre.*

**Lina** *(blÃªme)* : "847 dollars... en douze jours."

*Ses mains tremblent lÃ©gÃ¨rement. C'est plus que son loyer. Elle plonge dans les logs, cherchant le coupable. Et elle le trouve : 50,000 tokens par requÃªte en moyenne. Des fichiers entiers envoyÃ©s et renvoyÃ©s. Des outputs bash de 500 lignes reproduits dix fois. L'historique complet de chaque conversation, accumulÃ© comme des couches gÃ©ologiques.*

**Lina** *(la voix serrÃ©e)* : "Je paie pour envoyer les mÃªmes 500 lignes de logs npm Ã  chaque requÃªte. Le modÃ¨le n'en a besoin qu'une fois."

*Marc arrive avec deux cafÃ©s. Il jette un Å“il Ã  l'Ã©cran et grimace.*

**Marc** : "AÃ¯e. Le piÃ¨ge classique. Tu sais ce qui est ironique ?"

**Lina** : "Quoi ?"

**Marc** : "Les chercheurs de JetBrains ont dÃ©couvert quelque chose de contre-intuitif l'annÃ©e derniÃ¨re. Ils pensaient qu'envoyer plus de contexte amÃ©liorerait les rÃ©sultats de gÃ©nÃ©ration de code. Ils ont testÃ©. Et ils ont trouvÃ© l'inverse."

**Lina** *(levant les yeux)* : "L'inverse ?"

**Marc** : "Moins de contexte, mais mieux ciblÃ©, donne de **meilleurs** rÃ©sultats. Pas juste moins cher â€” plus prÃ©cis. Le modÃ¨le se perd moins."

*Lina pose sa tasse. Une lueur d'espoir.*

**Lina** : "Donc si je compresse intelligemment... je peux Ã©conomiser ET avoir de meilleures rÃ©ponses ?"

**Marc** *(souriant)* : "Exactement. Ã‡a s'appelle la **compression de contexte**. Et pour les rÃ©sultats d'outils qui traÃ®nent dans l'historique, on utilise l'**observation masking** â€” on cache ce qui n'est plus pertinent, tout en gardant une trace qu'il existe."

*Lina ferme la facture. Dans ses yeux, la panique a cÃ©dÃ© la place Ã  la dÃ©termination.*

**Lina** : "Montre-moi. Chaque technique. Je veux diviser cette facture par trois."

**Marc** : "Par trois ? On va viser mieux que Ã§a."

---

## ğŸ“‹ Table des matiÃ¨res

| Section | Titre | Description |
|:-------:|-------|-------------|
| 9.1 | ğŸ’¸ Le ProblÃ¨me du CoÃ»t | Pourquoi le contexte long est problÃ©matique |
| 9.2 | ğŸ—œï¸ Techniques de Compression | Vue d'ensemble des approches |
| 9.3 | âš–ï¸ Compression Priority-Based | Garder le critique, supprimer le bruit |
| 9.4 | ğŸ“ Summarization Intelligente | RÃ©sumer sans perdre l'essentiel |
| 9.5 | ğŸ­ Observation Masking | Cacher les outputs d'outils anciens |
| 9.6 | ğŸ› ï¸ ImplÃ©mentation | Le module dans Grok-CLI |
| 9.7 | ğŸ“Š MÃ©triques et Monitoring | Mesurer l'efficacitÃ© |
| 9.8 | ğŸ’¼ Cas Pratiques | Exemples concrets |

---

## 9.1 ğŸ’¸ Le ProblÃ¨me du Contexte Volumineux

### 9.1.1 Le coÃ»t rÃ©el du contexte

Chaque token envoyÃ© Ã  l'API coÃ»te de l'argent. Quand votre agent envoie 50K tokens par requÃªte, la facture grimpe vite.

![CoÃ»t par requÃªte](images/cost-per-request.svg)

### 9.1.2 Lost in the Middle â€” La DÃ©couverte qui a Tout ChangÃ©

Le coÃ»t n'est pas le seul problÃ¨me. Et ce qui suit est peut-Ãªtre la dÃ©couverte la plus importante sur les LLMs depuis les Transformers eux-mÃªmes.

**Ã‰tÃ© 2023, Stanford University.** Nelson Liu, un doctorant, pose une question simple Ã  son Ã©quipe : "Est-ce que la position d'une information dans le contexte affecte sa probabilitÃ© d'Ãªtre utilisÃ©e ?"

L'hypothÃ¨se semblait presque triviale. AprÃ¨s tout, les Transformers ont des mÃ©canismes d'attention qui sont censÃ©s regarder partout dans le contexte, non ?

Pour tester, ils ont crÃ©Ã© une expÃ©rience Ã©lÃ©gante : cacher un "fait clÃ©" Ã  diffÃ©rentes positions dans un contexte de 128K tokens, puis poser une question dont la rÃ©ponse nÃ©cessite ce fait.

**Les rÃ©sultats ont envoyÃ© des ondes de choc dans la communautÃ© IA.**

Quand le fait clÃ© Ã©tait au **dÃ©but** du contexte : 98% de rÃ©ponses correctes.
Quand il Ã©tait Ã  la **fin** : 95% de rÃ©ponses correctes.
Quand il Ã©tait **au milieu** : **45% de rÃ©ponses correctes**.

Le modÃ¨le "oubliait" littÃ©ralement ce qu'il avait lu au milieu du contexte. Ce phÃ©nomÃ¨ne, qu'ils ont baptisÃ© **"Lost in the Middle"**, affecte tous les LLMs â€” GPT-4, Claude, Llama, tous.

![Distribution de l'attention - Lost in the Middle](images/attention-distribution.svg)

| ProblÃ¨me | Impact | Solution |
|----------|--------|----------|
| ğŸ’¸ **CoÃ»t** | Factures Ã©levÃ©es | Compression |
| ğŸ¯ **Attention** | Info perdue au milieu | RÃ©organisation |
| â±ï¸ **Latence** | RÃ©ponses lentes | Moins de tokens |
| ğŸ­ **Dilution** | ModÃ¨le confus | Filtrage |

---

## 9.2 ğŸ—œï¸ Techniques de Compression

### 9.2.1 Vue d'ensemble

Il existe plusieurs techniques pour rÃ©duire la taille du contexte, chacune avec ses forces et faiblesses :

![Techniques de compression](images/compression-techniques.svg)

### 9.2.2 La DÃ©couverte de JetBrains (2024) â€” L'Histoire

> *"On pensait que plus de contexte serait toujours mieux. On avait tort."*
> â€” Ã‰quipe JetBrains Research, 2024

**L'histoire commence Ã  Saint-PÃ©tersbourg**, dans les bureaux de JetBrains â€” les crÃ©ateurs d'IntelliJ IDEA, PyCharm, et de Kotlin. Leur Ã©quipe de recherche en IA travaillait sur un problÃ¨me apparemment simple : comment amÃ©liorer la gÃ©nÃ©ration de code assistÃ©e par LLM dans leurs IDE ?

L'hypothÃ¨se initiale semblait Ã©vidente : **plus de contexte = meilleures suggestions**. AprÃ¨s tout, un dÃ©veloppeur qui voit tout le projet fait de meilleures suggestions qu'un qui ne voit qu'un fichier, non ?

Ils ont donc construit un systÃ¨me qui envoyait au LLM :
- Le fichier actuel complet
- Tous les fichiers importÃ©s
- L'historique de la session
- La documentation du projet
- Les tests associÃ©s

**Les rÃ©sultats les ont stupÃ©fiÃ©s.**

Non seulement les coÃ»ts avaient explosÃ©, mais la **qualitÃ© des suggestions avait diminuÃ©**. Le modÃ¨le se perdait dans la masse d'information. Il ignorait parfois le code juste avant le curseur pour citer de la documentation non pertinente situÃ©e 50,000 tokens plus tÃ´t.

C'est alors qu'ils ont eu l'idÃ©e de **mesurer systÃ©matiquement** l'impact de chaque type de contexte. Ils ont crÃ©Ã© un benchmark avec des centaines de tÃ¢ches de complÃ©tion de code, et ont testÃ© diffÃ©rentes stratÃ©gies de compression.

**Les rÃ©sultats publiÃ©s en 2024 :**

| Technique | RÃ©duction tokens | Impact succÃ¨s | CoÃ»t relatif |
|-----------|:----------------:|:-------------:|:------------:|
| Sans compression | 0% | Baseline | 100% |
| Priority-based | -40% | +1.2% âœ… | 60% |
| + Summarization | -55% | +2.1% âœ… | 45% |
| + Semantic dedup | -62% | +2.6% âœ… | 38% |
| Observation masking | -35% | +1.8% âœ… | 65% |
| **CombinÃ©** | **-70%** | **+2.6%** âœ… | **30%** |

> ğŸ’¡ **La conclusion qui a choquÃ© la communautÃ©** : Envoyer 70% de contexte en moins amÃ©liore la qualitÃ© de 2.6%. Ce n'est pas un compromis â€” c'est un gain sur les deux tableaux.

**Pourquoi ?** L'Ã©tude identifie trois mÃ©canismes :

1. **Attention focalisÃ©e** : Avec moins de contexte, chaque token a plus de poids dans le calcul d'attention
2. **RÃ©duction du bruit** : Les informations non pertinentes ne peuvent plus "distraire" le modÃ¨le
3. **CohÃ©rence amÃ©liorÃ©e** : Le modÃ¨le ne se contredit plus en citant des parties obsolÃ¨tes du contexte

Cette dÃ©couverte a depuis Ã©tÃ© confirmÃ©e par d'autres Ã©quipes (Google DeepMind, Anthropic), et a donnÃ© naissance Ã  une nouvelle discipline : **l'ingÃ©nierie de contexte**.

---

## 9.3 âš–ï¸ Compression Priority-Based

### 9.3.1 SystÃ¨me de prioritÃ©s

L'idÃ©e est simple : tout le contenu n'a pas la mÃªme importance. On dÃ©finit des niveaux de prioritÃ© :

```typescript
// src/context/context-compressor.ts

enum Priority {
  CRITICAL = 4,   // ğŸ”´ Toujours garder
  HIGH = 3,       // ğŸŸ  Garder si possible
  MEDIUM = 2,     // ğŸŸ¡ Peut Ãªtre rÃ©sumÃ©
  LOW = 1,        // ğŸŸ¢ Peut Ãªtre supprimÃ©
  NOISE = 0       // âš« Supprimer systÃ©matiquement
}

interface PrioritizedContent {
  content: string;
  type: ContentType;
  priority: Priority;
  tokens: number;
  timestamp?: Date;
  relevanceScore?: number;
}
```

![Pyramide des prioritÃ©s de contexte](images/priority-pyramid.svg)

### 9.3.2 Classification automatique

```typescript
// src/context/classifier.ts

/**
 * Classifie automatiquement le contenu par prioritÃ©.
 */
function classifyContent(content: PrioritizedContent): Priority {
  switch (content.type) {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸ”´ CRITICAL : Toujours nÃ©cessaire
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    case 'system_prompt':
      return Priority.CRITICAL;
    case 'current_user_message':
      return Priority.CRITICAL;
    case 'tool_call_in_progress':
      return Priority.CRITICAL;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸŸ  HIGH : TrÃ¨s important
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    case 'recent_code_context':
      return Priority.HIGH;
    case 'recent_tool_result':
      return Priority.HIGH;
    case 'error_message':
      return Priority.HIGH;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸŸ¡ MEDIUM : Important mais compressible
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    case 'older_conversation':
      return Priority.MEDIUM;
    case 'documentation':
      return Priority.MEDIUM;
    case 'test_output':
      return Priority.MEDIUM;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸŸ¢ LOW : Peut Ãªtre supprimÃ© si nÃ©cessaire
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    case 'verbose_logs':
      return Priority.LOW;
    case 'old_conversation':
      return Priority.LOW;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // âš« NOISE : Supprimer systÃ©matiquement
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    case 'progress_bars':
      return Priority.NOISE;
    case 'timestamps_repeated':
      return Priority.NOISE;
    case 'empty_lines':
      return Priority.NOISE;

    default:
      return Priority.MEDIUM;
  }
}
```

### 9.3.3 Algorithme de compression

```typescript
// src/context/context-compressor.ts

export class ContextCompressor {
  private tokenEncoder: TokenEncoder;
  private summarizer: Summarizer;

  /**
   * Compresse un ensemble de contenus pour respecter un budget tokens.
   * Algorithme :
   * 1. Trier par prioritÃ© (descending)
   * 2. Supprimer le NOISE
   * 3. Ajouter par ordre de prioritÃ© jusqu'au budget
   * 4. RÃ©sumer les MEDIUM si nÃ©cessaire
   * 5. Tronquer les HIGH si vraiment nÃ©cessaire
   */
  async compress(
    contents: PrioritizedContent[],
    maxTokens: number
  ): Promise<CompressedContext> {
    // 1ï¸âƒ£ Classifier et trier par prioritÃ©
    const classified = contents.map(c => ({
      ...c,
      priority: classifyContent(c)
    }));
    classified.sort((a, b) => b.priority - a.priority);

    // 2ï¸âƒ£ Supprimer le NOISE
    const withoutNoise = classified.filter(c => c.priority > Priority.NOISE);

    // 3ï¸âƒ£ Calculer les tokens actuels
    const originalTokens = withoutNoise.reduce((sum, c) => sum + c.tokens, 0);

    // 4ï¸âƒ£ Si sous la limite, retourner tel quel
    if (originalTokens <= maxTokens) {
      return {
        contents: withoutNoise,
        originalTokens,
        compressedTokens: originalTokens,
        compressionRatio: 1.0
      };
    }

    // 5ï¸âƒ£ Compression itÃ©rative
    const result: PrioritizedContent[] = [];
    let usedTokens = 0;

    for (const content of classified) {
      if (content.priority === Priority.NOISE) continue;

      const remainingTokens = maxTokens - usedTokens;

      if (content.tokens <= remainingTokens) {
        // âœ… Ã‡a rentre, ajouter tel quel
        result.push(content);
        usedTokens += content.tokens;

      } else if (content.priority >= Priority.HIGH) {
        // ğŸŸ  Critique/High : tronquer plutÃ´t que supprimer
        const truncated = await this.truncate(content, remainingTokens);
        result.push(truncated);
        usedTokens += truncated.tokens;

      } else if (content.priority === Priority.MEDIUM && remainingTokens > 100) {
        // ğŸŸ¡ Medium : rÃ©sumer
        const summarized = await this.summarize(content, remainingTokens);
        result.push(summarized);
        usedTokens += summarized.tokens;
      }
      // ğŸŸ¢ LOW : skip si pas de place
    }

    return {
      contents: result,
      originalTokens,
      compressedTokens: usedTokens,
      compressionRatio: usedTokens / originalTokens
    };
  }

  private async truncate(
    content: PrioritizedContent,
    maxTokens: number
  ): Promise<PrioritizedContent> {
    const tokens = this.tokenEncoder.encode(content.content);
    const truncatedTokens = tokens.slice(0, maxTokens - 20);
    const truncatedText = this.tokenEncoder.decode(truncatedTokens);

    return {
      ...content,
      content: truncatedText + '\n[... truncated ...]',
      tokens: truncatedTokens.length + 5
    };
  }

  private async summarize(
    content: PrioritizedContent,
    maxTokens: number
  ): Promise<PrioritizedContent> {
    const summary = await this.summarizer.summarize(content.content, {
      maxLength: maxTokens - 10,
      preserveCode: content.type.includes('code'),
      preserveErrors: content.type.includes('error')
    });

    return {
      ...content,
      content: `[Summary] ${summary}`,
      tokens: this.tokenEncoder.encode(summary).length + 3
    };
  }
}
```

---

## 9.4 ğŸ“ Summarization Intelligente

### 9.4.1 RÃ©sumer la conversation

Les conversations longues peuvent Ãªtre rÃ©sumÃ©es tout en prÃ©servant les informations clÃ©s :

```typescript
// src/context/summarizer.ts

/**
 * RÃ©sume une conversation longue.
 * Garde les N derniers messages intacts et rÃ©sume le reste.
 */
async function summarizeConversation(
  messages: Message[],
  maxTokens: number
): Promise<string> {
  // Garder les N derniers messages intacts
  const recentCount = 4;
  const recent = messages.slice(-recentCount);
  const older = messages.slice(0, -recentCount);

  if (older.length === 0) {
    return formatMessages(recent);
  }

  // RÃ©sumer les anciens messages avec un LLM
  const olderText = formatMessages(older);
  const summaryPrompt = `
RÃ©sume cette conversation en gardant UNIQUEMENT :
- Les dÃ©cisions prises
- Les fichiers modifiÃ©s
- Les erreurs rencontrÃ©es
- Les tÃ¢ches complÃ©tÃ©es

Conversation Ã  rÃ©sumer :
${olderText}

RÃ©sumÃ© (max 200 mots) :
  `;

  const summary = await llm.complete(summaryPrompt, { maxTokens: 300 });

  return `
[ğŸ“ RÃ©sumÃ© des ${older.length} messages prÃ©cÃ©dents]
${summary}

[ğŸ’¬ Messages rÃ©cents]
${formatMessages(recent)}
  `.trim();
}
```

### 9.4.2 RÃ©sumer les rÃ©sultats d'outils

Chaque outil a des patterns spÃ©cifiques Ã  rÃ©sumer :

```typescript
// src/context/tool-summarizer.ts

/**
 * RÃ©sume intelligemment le rÃ©sultat d'un outil.
 * StratÃ©gies diffÃ©rentes selon le type d'outil.
 */
async function summarizeToolResult(
  toolName: string,
  result: string,
  maxTokens: number
): Promise<string> {
  const resultTokens = countTokens(result);

  if (resultTokens <= maxTokens) {
    return result;  // Pas besoin de rÃ©sumer
  }

  // StratÃ©gies spÃ©cifiques par outil
  switch (toolName) {
    case 'bash':
      return summarizeBashOutput(result, maxTokens);
    case 'read_file':
      return summarizeFileContent(result, maxTokens);
    case 'search':
      return summarizeSearchResults(result, maxTokens);
    case 'list_directory':
      return summarizeDirectoryListing(result, maxTokens);
    default:
      return genericSummarize(result, maxTokens);
  }
}

/**
 * RÃ©sume un output bash en gardant les erreurs et les derniÃ¨res lignes.
 */
function summarizeBashOutput(output: string, maxTokens: number): string {
  const lines = output.split('\n');

  // Extraire par prioritÃ©
  const errorLines = lines.filter(l => l.match(/error|fail|exception/i));
  const warningLines = lines.filter(l => l.match(/warn/i));
  const lastLines = lines.slice(-20);

  // Combiner sans doublons
  const prioritized = [...new Set([
    ...errorLines.slice(0, 10),
    ...warningLines.slice(0, 5),
    ...lastLines
  ])];

  const result = prioritized.join('\n');

  if (countTokens(result) <= maxTokens) {
    return `[ğŸ“Š Output: ${lines.length} lignes â†’ ${prioritized.length} lignes]\n${result}`;
  }

  // Tronquer si encore trop long
  return truncateToTokens(result, maxTokens);
}
```

| Outil | StratÃ©gie de rÃ©sumÃ© | Ce qu'on garde |
|-------|---------------------|----------------|
| `bash` | PrioritÃ© erreurs | Errors > Warnings > Last 20 lines |
| `read_file` | Structure + highlights | Imports, classes, fonctions clÃ©s |
| `search` | Top N matches | Premiers rÃ©sultats pertinents |
| `list_directory` | Stats + structure | Nombre de fichiers, types |

---

## 9.5 ğŸ­ Observation Masking

### 9.5.1 Le problÃ¨me des outputs verbeux

Quand un outil retourne un gros rÃ©sultat, ce rÃ©sultat reste dans le contexte pour TOUTES les requÃªtes suivantes â€” mÃªme quand il n'est plus pertinent.

![Observation Masking](images/observation-masking.svg)

### 9.5.2 CritÃ¨res de masquage

```typescript
// src/context/observation-masking.ts

interface MaskingCriteria {
  maxAge: number;              // Masquer aprÃ¨s N messages
  minTokensToMask: number;     // Ne masquer que si > N tokens
  relevanceThreshold: number;  // Score de pertinence minimum
  toolSpecificRules: Record<string, ToolMaskingRule>;
}

interface ToolMaskingRule {
  alwaysMaskAfter?: number;    // Masquer aprÃ¨s N messages
  keepSummary?: boolean;       // Garder un rÃ©sumÃ©
  keepMatches?: number;        // Garder les N premiers rÃ©sultats
  keepIfReferenced?: boolean;  // Garder si rÃ©fÃ©rencÃ© rÃ©cemment
  maskProgressBars?: boolean;  // Masquer les barres de progression
  keepErrors?: boolean;        // Toujours garder les erreurs
}

const DEFAULT_CRITERIA: MaskingCriteria = {
  maxAge: 5,              // Masquer aprÃ¨s 5 messages
  minTokensToMask: 500,   // Masquer si > 500 tokens
  relevanceThreshold: 0.3,

  toolSpecificRules: {
    'list_directory': {
      alwaysMaskAfter: 2,
      keepSummary: true
    },
    'search': {
      alwaysMaskAfter: 3,
      keepMatches: 5
    },
    'read_file': {
      alwaysMaskAfter: 5,
      keepIfReferenced: true
    },
    'bash': {
      maskProgressBars: true,
      keepErrors: true
    }
  }
};
```

### 9.5.3 ImplÃ©mentation

```typescript
// src/context/observation-masking.ts

export class ObservationMasker {
  private criteria: MaskingCriteria;

  /**
   * DÃ©termine si un rÃ©sultat d'outil doit Ãªtre masquÃ©.
   */
  shouldMask(
    toolResult: ToolResult,
    currentMessageIndex: number,
    context: ConversationContext
  ): MaskDecision {
    const age = currentMessageIndex - toolResult.messageIndex;
    const tokens = countTokens(toolResult.output);

    // ğŸ“ RÃ¨gle 1 : Ã‚ge
    if (age > this.criteria.maxAge) {
      return { mask: true, reason: 'age', keepSummary: true };
    }

    // ğŸ“ RÃ¨gle 2 : Trop petit pour valoir la peine
    if (tokens < this.criteria.minTokensToMask) {
      return { mask: false };
    }

    // ğŸ“ RÃ¨gle 3 : RÃ¨gles spÃ©cifiques Ã  l'outil
    const toolRule = this.criteria.toolSpecificRules[toolResult.toolName];
    if (toolRule?.alwaysMaskAfter && age > toolRule.alwaysMaskAfter) {
      return {
        mask: true,
        reason: 'tool_rule',
        keepSummary: toolRule.keepSummary,
        keepMatches: toolRule.keepMatches
      };
    }

    // ğŸ“ RÃ¨gle 4 : Pertinence
    const relevance = this.computeRelevance(toolResult, context.currentMessage);
    if (relevance < this.criteria.relevanceThreshold) {
      return { mask: true, reason: 'low_relevance', keepSummary: true };
    }

    return { mask: false };
  }

  /**
   * GÃ©nÃ¨re la version masquÃ©e d'un rÃ©sultat.
   */
  mask(toolResult: ToolResult, decision: MaskDecision): string {
    if (!decision.mask) {
      return toolResult.output;
    }

    const summary = this.generateSummary(toolResult, decision);

    return `[ğŸ­ MASKED: ${toolResult.toolName}]
${summary}
[Full output in message #${toolResult.messageIndex}]`;
  }

  private generateSummary(
    toolResult: ToolResult,
    decision: MaskDecision
  ): string {
    const output = toolResult.output;

    switch (toolResult.toolName) {
      case 'list_directory':
        const fileCount = (output.match(/\n/g) || []).length;
        return `ğŸ“ Listed ${fileCount} files/directories`;

      case 'search':
        const matchCount = (output.match(/:\d+:/g) || []).length;
        if (decision.keepMatches) {
          const firstMatches = output
            .split('\n')
            .slice(0, decision.keepMatches)
            .join('\n');
          return `ğŸ” Found ${matchCount} matches:\n${firstMatches}`;
        }
        return `ğŸ” Found ${matchCount} matches`;

      case 'bash':
        const lines = output.split('\n').length;
        const hasError = /error|fail/i.test(output);
        return `âš¡ Executed (${lines} lines${hasError ? ', âŒ contains errors' : ''})`;

      case 'read_file':
        const lineCount = output.split('\n').length;
        return `ğŸ“„ File content (${lineCount} lines)`;

      default:
        const tokens = countTokens(output);
        return `ğŸ“‹ Result (${tokens} tokens)`;
    }
  }
}
```

---

## 9.6 ğŸ› ï¸ ImplÃ©mentation Grok-CLI

### 9.6.1 Architecture du module

![Architecture Compression](images/compression-architecture.svg)

### 9.6.2 IntÃ©gration dans l'agent

```typescript
// src/agent/grok-agent.ts

export class GrokAgent {
  private compressor: ContextCompressor;
  private masker: ObservationMasker;
  private tokenBudget: number = 100_000;

  /**
   * Construit le contexte optimisÃ© pour une requÃªte.
   */
  async buildContext(messages: Message[]): Promise<Context> {
    // 1ï¸âƒ£ Classifier les messages
    const classified = messages.map(m => this.classifyMessage(m));

    // 2ï¸âƒ£ Masquer les observations anciennes/non pertinentes
    const masked = this.applyMasking(classified);

    // 3ï¸âƒ£ Compresser pour respecter le budget
    const compressed = await this.compressor.compress(
      masked,
      this.tokenBudget
    );

    // 4ï¸âƒ£ Optimiser l'ordre (Ã©viter "lost in the middle")
    const optimized = this.optimizeOrder(compressed.contents);

    return {
      messages: optimized,
      stats: {
        originalTokens: compressed.originalTokens,
        compressedTokens: compressed.compressedTokens,
        compressionRatio: compressed.compressionRatio,
        maskedObservations: masked.filter(m => m.masked).length
      }
    };
  }

  /**
   * RÃ©organise le contenu pour maximiser l'attention.
   * StratÃ©gie : CRITICAL au dÃ©but, HIGH ensuite, reste intercalÃ©.
   */
  private optimizeOrder(contents: PrioritizedContent[]): PrioritizedContent[] {
    const critical = contents.filter(c => c.priority === Priority.CRITICAL);
    const high = contents.filter(c => c.priority === Priority.HIGH);
    const rest = contents.filter(c => c.priority < Priority.HIGH);

    // Intercaler le reste pour Ã©viter le "lost in the middle"
    const interleavedRest: PrioritizedContent[] = [];
    const mid = Math.floor(rest.length / 2);

    for (let i = 0; i < mid; i++) {
      interleavedRest.push(rest[i]);
      if (rest[mid + i]) {
        interleavedRest.push(rest[mid + i]);
      }
    }

    return [...critical, ...high, ...interleavedRest];
  }
}
```

### 9.6.3 Configuration

```typescript
// src/context/config.ts

export const COMPRESSION_CONFIG = {
  // ğŸ“Š Budgets
  defaultTokenBudget: 100_000,
  maxTokenBudget: 128_000,

  // ğŸ—œï¸ Compression
  enableCompression: true,
  compressionThreshold: 0.8,  // Compresser si > 80% du budget

  // ğŸ­ Masking
  enableMasking: true,
  maskingCriteria: {
    maxAge: 5,
    minTokensToMask: 500,
    relevanceThreshold: 0.3
  },

  // ğŸ“ Summarization
  enableSummarization: true,
  summarizeConversationAfter: 10,  // messages
  maxSummaryTokens: 500,

  // âš–ï¸ PrioritÃ©s par type
  priorities: {
    system_prompt: Priority.CRITICAL,
    current_user_message: Priority.CRITICAL,
    recent_tool_result: Priority.HIGH,
    error_message: Priority.HIGH,
    code_context: Priority.HIGH,
    older_conversation: Priority.MEDIUM,
    verbose_output: Priority.LOW
  }
};
```

---

## 9.7 ğŸ“Š MÃ©triques et Monitoring

### 9.7.1 Dashboard de compression

```typescript
// src/context/metrics.ts

interface CompressionMetrics {
  // Par session
  totalOriginalTokens: number;
  totalCompressedTokens: number;
  avgCompressionRatio: number;
  totalMaskedObservations: number;

  // Par message
  messagesProcessed: number;
  summarizationsPerformed: number;

  // Ã‰conomies
  estimatedCostSaved: number;
}

function printCompressionDashboard(metrics: CompressionMetrics): void {
  // Affiche le dashboard de compression
  // Voir images/compression-dashboard.svg pour la visualisation
}
```

### 9.7.2 Alertes de santÃ©

```typescript
function checkCompressionHealth(metrics: CompressionMetrics): Alert[] {
  const alerts: Alert[] = [];

  // âš ï¸ Compression trop agressive
  if (metrics.avgCompressionRatio < 0.3) {
    alerts.push({
      level: 'warning',
      message: 'âš ï¸ Compression trÃ¨s agressive (< 30%), risque de perte d\'info'
    });
  }

  // â„¹ï¸ Pas assez de compression
  if (metrics.avgCompressionRatio > 0.95) {
    alerts.push({
      level: 'info',
      message: 'â„¹ï¸ Compression minimale, budget peut-Ãªtre trop Ã©levÃ©'
    });
  }

  // âš ï¸ Trop de summarizations
  if (metrics.summarizationsPerformed > metrics.messagesProcessed * 0.5) {
    alerts.push({
      level: 'warning',
      message: 'âš ï¸ Beaucoup de rÃ©sumÃ©s, messages peut-Ãªtre trop longs'
    });
  }

  return alerts;
}
```

---

## 9.8 ğŸ’¼ Cas Pratiques

### Cas 1 : Session longue

![Cas Session Longue](images/case-session.svg)

### Cas 2 : Recherche massive

![Cas Recherche Massive](images/case-search.svg)

### Cas 3 : Logs verbeux

![Cas Logs Verbeux](images/case-logs.svg)

---

## ğŸ“ Points ClÃ©s

| Concept | Point clÃ© |
|---------|-----------|
| ğŸ’¸ **ProblÃ¨me** | Contexte long = cher, lent, imprÃ©cis |
| âš–ï¸ **Priority-based** | Garder le critique, compresser le reste |
| ğŸ“ **Summarization** | RÃ©sumer les parties longues |
| ğŸ­ **Observation masking** | Cacher les outputs d'outils anciens |
| ğŸ“Š **Token budget** | Respecter une limite stricte |
| ğŸ§  **Lost in the Middle** | Placer l'important au dÃ©but/fin |
| ğŸ“ˆ **RÃ©sultats** | -70% tokens, +2.6% succÃ¨s |

---

## âš ï¸ 9.8 Limites et Risques

### ğŸš§ Limites Techniques

| Limite | Description | Impact |
|--------|-------------|--------|
| **Perte d'information** | Compression = suppression | DÃ©tails importants potentiellement perdus |
| **QualitÃ© du rÃ©sumÃ©** | DÃ©pend du LLM de summarization | RÃ©sumÃ©s parfois incomplets |
| **Latence ajoutÃ©e** | Classification + compression = temps | RÃ©ponse initiale plus lente |
| **Masquage trop agressif** | Informations nÃ©cessaires cachÃ©es | RÃ©ponses incomplÃ¨tes |
| **Calibration des prioritÃ©s** | DÃ©pend du domaine/workflow | Configuration nÃ©cessaire |

### âš¡ Risques OpÃ©rationnels

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|:-----------:|:------:|------------|
| **Sur-compression** | Moyenne | Ã‰levÃ© | Seuil de compression conservateur (0.7) |
| **Masquage de contexte critique** | Faible | Critique | Exceptions pour erreurs et code rÃ©cent |
| **IncohÃ©rence du rÃ©sumÃ©** | Moyenne | Moyen | Validation du rÃ©sumÃ© par le LLM |
| **DÃ©gradation de la qualitÃ©** | Faible | Moyen | Monitoring du taux de succÃ¨s |

### ğŸ“Š Quand NE PAS Compresser

| Situation | Raison | Action |
|-----------|--------|--------|
| Contexte < 50% du budget | Pas nÃ©cessaire | Skip compression |
| Debugging critique | Besoin de tous les dÃ©tails | Mode verbose |
| PremiÃ¨re interaction | Pas encore de contexte | Rien Ã  compresser |

> ğŸ“Œ **Ã€ Retenir** : La compression de contexte est un **compromis Ã©conomique** â€” on Ã©change des tokens (donc du coÃ»t et de la capacitÃ©) contre une potentielle perte d'information. L'art est de trouver le point oÃ¹ on gagne plus qu'on ne perd. En pratique, une compression de 50-70% amÃ©liore souvent les rÃ©sultats en forÃ§ant le modÃ¨le Ã  se concentrer sur l'essentiel.

> ğŸ’¡ **Astuce Pratique** : Activez le masquage des observations d'abord (gain facile, peu de risque), puis la summarization (gain modÃ©rÃ©, risque modÃ©rÃ©), puis la troncation (dernier recours).

---

## ğŸ“Š Tableau SynthÃ©tique â€” Chapitre 09

| Aspect | DÃ©tails |
|--------|---------|
| **Titre** | Context Compression |
| **ProblÃ¨me** | Contexte explose â†’ coÃ»ts et "Lost in the Middle" |
| **Solution** | Classification + compression intelligente |
| **PrioritÃ©s** | CRITICAL > HIGH > MEDIUM > LOW |
| **Techniques** | Masking, Summarization, Truncation |
| **"Lost in the Middle"** | Placer l'important au dÃ©but/fin |
| **RÃ©sultats** | -70% tokens, +2.6% succÃ¨s |
| **Papier de RÃ©fÃ©rence** | JetBrains Research (2024) |

---

## ğŸ‹ï¸ Exercices

### Exercice 1 : SystÃ¨me de prioritÃ©s
**Objectif** : DÃ©finir vos prioritÃ©s

| Type de contenu | PrioritÃ© | Justification |
|-----------------|:--------:|---------------|
| System prompt | | |
| Message utilisateur actuel | | |
| RÃ©sultat d'erreur | | |
| Logs npm | | |
| Conversation d'hier | | |

### Exercice 2 : RÃ¨gles de masking
**Objectif** : ImplÃ©menter des rÃ¨gles pour votre workflow

```typescript
const myMaskingRules: Record<string, ToolMaskingRule> = {
  'my_custom_tool': {
    alwaysMaskAfter: ???,
    keepSummary: ???,
    keepErrors: ???
  }
};
```

### Exercice 3 : Benchmark qualitÃ©
**Objectif** : Mesurer l'impact sur la qualitÃ©

| Question | Sans compression | Avec compression | DiffÃ©rence |
|----------|:----------------:|:----------------:|:----------:|
| Q1 | | | |
| Q2 | | | |
| ... | | | |

### Exercice 4 : Trouver le ratio optimal
**Objectif** : Ã‰quilibre coÃ»t/qualitÃ©

| Compression | CoÃ»t | QualitÃ© | Score |
|:-----------:|:----:|:-------:|:-----:|
| 0% | | | |
| 30% | | | |
| 50% | | | |
| 70% | | | |

---

## ğŸ“š RÃ©fÃ©rences

| Type | RÃ©fÃ©rence |
|------|-----------|
| ğŸ“„ Paper | JetBrains Research. (2024). "Context Compression for LLM-based Code Generation" |
| ğŸ“„ Paper | Liu, N., et al. (2023). "Lost in the Middle: How Language Models Use Long Contexts" |
| ğŸ’» Code | Grok-CLI : `src/context/context-compressor.ts` |
| ğŸ’» Code | Grok-CLI : `src/context/observation-masking.ts` |

---

## ğŸŒ… Ã‰pilogue â€” Le Prix de l'Attention

*Un mois plus tard. 23h45. Lina fixe sa nouvelle facture API.*

**Lina** *(un sourire se dessinant)* : "253 dollars."

*Elle fait le calcul dans sa tÃªte. 847 dollars le mois dernier. 253 maintenant. Presque 70% de moins.*

**Marc** *(levant les yeux de son Ã©cran)* : "Et les rÃ©ponses ?"

**Lina** : "C'est Ã§a le plus fou. Elles sont meilleures. Vraiment meilleures."

*Elle pivote son Ã©cran vers lui. Un log de session, annotÃ©.*

**Lina** : "Regarde. Avant, quand je demandais de corriger un bug, l'agent citait parfois de la documentation obsolÃ¨te qu'il avait lue 20 messages plus tÃ´t. Maintenant, il va droit au code pertinent."

**Marc** : "Le paradoxe de JetBrains. Moins de contexte, mais mieux ciblÃ©. Le modÃ¨le n'a plus Ã  choisir oÃ¹ regarder parmi 150,000 tokens. On a fait ce choix pour lui."

*Un silence. Lina se mord la lÃ¨vre, pensive.*

**Lina** : "Marc... J'ai une question qui me trotte dans la tÃªte depuis quelques jours."

**Marc** : "Hmm ?"

**Lina** : "On optimise le contexte. On optimise la mÃ©moire. On a mÃªme un RAG avec dÃ©pendances. Mais... l'agent a 41 outils Ã  sa disposition. 41. Comment il sait lequel utiliser ?"

*Marc pose son cafÃ©. Son expression change â€” un mÃ©lange de satisfaction et d'anticipation, comme un professeur dont l'Ã©lÃ¨ve vient de poser exactement la bonne question.*

**Marc** : "Ah. Tu touches Ã  quelque chose de fondamental lÃ ."

**Lina** : "C'est juste que... parfois je le vois hÃ©siter. Ou pire, utiliser `bash` pour quelque chose que `read_file` ferait mieux. Ou faire trois appels sÃ©quentiels quand il pourrait parallÃ©liser."

**Marc** : "Tu as remarquÃ© Ã§a ?"

**Lina** : "Difficile de ne pas le remarquer quand on regarde la facture en dÃ©tail."

*Marc se lÃ¨ve, va au tableau blanc, et dessine un schÃ©ma.*

**Marc** : "Les outils sont le **systÃ¨me nerveux** de l'agent. Tout ce qu'on a construit â€” le reasoning, la mÃ©moire, le contexte â€” tout Ã§a converge vers un moment critique : le **tool call**."

*Il trace une flÃ¨che.*

**Marc** : "C'est lÃ  que l'intention devient action. Et c'est lÃ  que la plupart des agents Ã©chouent."

**Lina** *(intriguÃ©e)* : "Comment Ã§a ?"

**Marc** : "Un outil mal choisi, c'est du temps perdu et de l'argent gaspillÃ©. Un outil mal paramÃ©trÃ©, c'est une erreur Ã  corriger. Un outil exÃ©cutÃ© sans validation... c'est un risque de sÃ©curitÃ©."

*Il se retourne vers elle, une lueur dans les yeux.*

**Marc** : "Tu veux vraiment comprendre comment fonctionne un agent LLM ?"

**Lina** : "Ã‰videmment."

**Marc** : "Alors il est temps de plonger dans le **Tool-Use**. Le vrai. Pas juste 'appeler une fonction'. On va parler de validation de schÃ©ma, de permissions, de confirmation utilisateur, d'exÃ©cution parallÃ¨le... et de ce qui se passe quand un outil Ã©choue."

*Lina ferme la facture et ouvre un nouveau fichier.*

**Lina** : "Je suis prÃªte."

**Marc** *(souriant)* : "Tu vas adorer. Et dÃ©tester. Probablement les deux en mÃªme temps."

*Il Ã©crit au tableau : "41 outils. 1 dÃ©cision. 0 marge d'erreur."*

---

*Fin de la Partie III â€” MÃ©moire, RAG et Contexte*

*Dans le prochain chapitre : Comment transformer une intention en action â€” sans casser quoi que ce soit.*

---

<div align="center">

**â† [Chapitre 8 : Dependency-Aware RAG](08-dependency-aware-rag.md)** | **[Sommaire](README.md)** | **[Chapitre 10 : Tool-Use](10-tool-use.md) â†’**

</div>
