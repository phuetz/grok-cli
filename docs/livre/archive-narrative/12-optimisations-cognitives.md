# Chapitre 12 â€” Optimisations Cognitives ğŸ§ 

---

## ğŸ¬ ScÃ¨ne d'ouverture

*Vendredi soir, 19h30. La plupart des bureaux sont dÃ©jÃ  vides. Lina, elle, fixe son Ã©cran avec une obsession croissante.*

*Sur son moniteur, un graphique en temps rÃ©el. Chaque seconde, une nouvelle requÃªte apparaÃ®t. Elle a commencÃ© Ã  les colorer mentalement : bleu pour les nouvelles, orange pour les "dÃ©jÃ  vues".*

*Orange. Orange. Bleu. Orange. Orange. Orange.*

**Lina** *(murmurant)* : "C'est pas possible..."

*Elle attrape son carnet et commence Ã  noter. Dix minutes plus tard, elle a son verdict.*

**Lina** : "68%. 68% de mes requÃªtes API sont des variations de la mÃªme chose."

*Marc passe derriÃ¨re elle, sa veste dÃ©jÃ  sur l'Ã©paule.*

**Marc** : "Tu comptes rester tard un vendredi ?"

**Lina** *(sans se retourner)* : "Regarde Ã§a."

*Elle lui montre son carnet. Une colonne de requÃªtes, avec des flÃ¨ches reliant celles qui sont Ã©quivalentes.*

```
"Comment lister les fichiers ?"
"ls"
"Montre-moi le contenu du dossier"
"Affiche les fichiers"
"Que contient ce rÃ©pertoire ?"
```

**Marc** *(posant sa veste)* : "Cinq faÃ§ons de poser la mÃªme question."

**Lina** : "Et mon agent appelle l'API cinq fois. Ã€ $0.03 par requÃªte, Ã§a fait $15 par jour perdus sur des questions dont il connaÃ®t dÃ©jÃ  la rÃ©ponse. $450 par mois. $5,400 par an."

*Elle se retourne enfin.*

**Lina** : "C'est plus que mon premier salaire de stage."

**Marc** *(s'asseyant)* : "Tu sais ce qui est frustrant ? Le cerveau humain rÃ©sout ce problÃ¨me naturellement. Tu ne 're-rÃ©flÃ©chis' pas Ã  comment faire du cafÃ© chaque matin."

**Lina** : "Exactement ! J'ai besoin d'un cache. Mais pas un cache bÃªte qui compare des strings caractÃ¨re par caractÃ¨re."

**Marc** : "Un cache qui comprend que 'ls' et 'lister les fichiers' veulent dire la mÃªme chose..."

**Lina** *(les yeux brillants)* : "Un cache **sÃ©mantique**. Qui compare le sens, pas les mots."

*Marc sourit. Il retire sa veste.*

**Marc** : "Ok. Je reste. On va construire quelque chose d'Ã©lÃ©gant."

---

## ğŸ“‹ Table des MatiÃ¨res

| Section | Titre | Description |
|:-------:|-------|-------------|
| 12.1 | ğŸ’¸ Le CoÃ»t de la Redondance | Analyse des patterns de requÃªtes |
| 12.2 | ğŸ”® Semantic Response Cache | Caching basÃ© sur la similaritÃ© |
| 12.3 | ğŸ”§ Tool Result Cache | Caching des rÃ©sultats d'outils |
| 12.4 | âš¡ PrÃ©-calcul et Warming | Anticipation des besoins |
| 12.5 | ğŸ“Š MÃ©triques et Monitoring | Dashboard d'optimisation |
| 12.6 | âœ… Bonnes Pratiques | Guidelines de caching |

---

## 12.1 ğŸ’¸ Le CoÃ»t de la Redondance

Un agent naÃ¯f appelle le LLM pour chaque requÃªte, mÃªme quand la rÃ©ponse a dÃ©jÃ  Ã©tÃ© calculÃ©e. Cette approche Â« sans mÃ©moire Â» gÃ©nÃ¨re un gaspillage considÃ©rable â€” en temps, en argent, et en ressources environnementales.

### 12.1.1 ğŸ” Analyse des Patterns de RequÃªtes

Avant d'optimiser, il faut mesurer. Une analyse sur une semaine d'utilisation typique rÃ©vÃ¨le un pattern frappant :

![Analyse des requÃªtes](images/request-analysis.svg)

Cette analyse rÃ©vÃ¨le que **68% des requÃªtes** (quasi-identiques + rÃ©pÃ©titions) pourraient Ãªtre servies depuis un cache, sans jamais toucher Ã  l'API.

### 12.1.2 ğŸ“Š Types de Redondance

Toutes les redondances ne se valent pas. Certaines sont faciles Ã  dÃ©tecter, d'autres nÃ©cessitent une comprÃ©hension sÃ©mantique :

| Type | IcÃ´ne | Exemple | DÃ©tection | Cache Possible |
|------|:-----:|---------|-----------|:--------------:|
| **Exact** | ğŸ“‹ | `"ls"` â†’ `"ls"` | Triviale | âœ… Simple |
| **SÃ©mantique** | ğŸ”® | `"liste les fichiers"` â†’ `"ls"` | Embeddings | âœ… SÃ©mantique |
| **ParamÃ©trique** | ğŸ”¢ | `"lis config.ts"` â†’ `"lis utils.ts"` | Template | âš ï¸ Partiel |
| **Contextuel** | ğŸ“ | MÃªme question, contexte diffÃ©rent | Impossible | âŒ Non |

**La clÃ©** : Un cache exact capture 20% des cas. Un cache sÃ©mantique en capture 68%.

### 12.1.3 ğŸ¯ Pourquoi 68% ?

Ce chiffre n'est pas arbitraire â€” il Ã©merge de patterns cognitifs prÃ©visibles :

![Patterns de redondance](images/redundancy-patterns.svg)

---

## 12.2 ğŸ”® Semantic Response Cache

Le **cache sÃ©mantique** est la technique la plus puissante pour rÃ©duire les appels API. Au lieu de chercher une correspondance exacte, il compare la *signification* des requÃªtes.

### 12.2.1 ğŸ“ Principe MathÃ©matique

L'idÃ©e est simple : deux requÃªtes qui signifient la mÃªme chose devraient avoir la mÃªme rÃ©ponse.

![Semantic Cache Flow](images/semantic-cache-flow.svg)

La **similaritÃ© cosine** mesure l'angle entre deux vecteurs :

```
                    A Â· B           Î£(aáµ¢ Ã— báµ¢)
cos(Î¸) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               ||A|| Ã— ||B||     âˆšÎ£aáµ¢Â² Ã— âˆšÎ£báµ¢Â²
```

- **cos = 1.0** : Vecteurs identiques (mÃªme direction)
- **cos = 0.0** : Vecteurs orthogonaux (aucune relation)
- **cos = -1.0** : Vecteurs opposÃ©s

En pratique, un seuil de **0.92** offre un bon Ã©quilibre entre hits et prÃ©cision.

### 12.2.2 ğŸ”§ ImplÃ©mentation ComplÃ¨te

```typescript
// src/utils/semantic-cache.ts
import { createHash } from 'crypto';
import { promises as fs } from 'fs';

/**
 * ğŸ“¦ Structure d'une entrÃ©e de cache
 * Stocke non seulement la rÃ©ponse, mais aussi les mÃ©tadonnÃ©es
 * nÃ©cessaires pour l'Ã©viction et l'analyse.
 */
interface CacheEntry {
  id: string;                    // ğŸ”‘ Identifiant unique
  query: string;                 // ğŸ“ RequÃªte originale
  queryEmbedding: number[];      // ğŸ§® Embedding de la requÃªte
  response: string;              // ğŸ’¬ RÃ©ponse cachÃ©e
  createdAt: Date;               // ğŸ“… Date de crÃ©ation
  accessCount: number;           // ğŸ“Š Nombre d'accÃ¨s
  lastAccess: Date;              // â° Dernier accÃ¨s
  metadata: {
    model: string;               // ğŸ¤– ModÃ¨le utilisÃ©
    tokens: number;              // ğŸ”¢ Tokens consommÃ©s
    context?: string;            // ğŸ“ Contexte optionnel
  };
}

/**
 * ğŸ“Š RÃ©sultat d'une recherche dans le cache
 */
interface CacheResult {
  response: string;              // ğŸ’¬ La rÃ©ponse
  similarity: number;            // ğŸ“ Score de similaritÃ©
  originalQuery: string;         // ğŸ“ RequÃªte qui a gÃ©nÃ©rÃ© cette rÃ©ponse
  metadata: CacheEntry['metadata'];
}

/**
 * ğŸ”® SemanticCache - Cache intelligent basÃ© sur la similaritÃ© sÃ©mantique
 *
 * Contrairement Ã  un cache exact (key â†’ value), ce cache trouve des
 * correspondances mÃªme quand les requÃªtes sont formulÃ©es diffÃ©remment.
 *
 * Exemple :
 * - "Comment lister les fichiers ?" â†’ embedding â†’ recherche
 * - Trouve "ls ou dir pour lister" avec similaritÃ© 0.94
 * - Retourne la rÃ©ponse cachÃ©e
 */
export class SemanticCache {
  private entries: Map<string, CacheEntry> = new Map();
  private embedder: Embedder;

  // âš™ï¸ Configuration
  private readonly similarityThreshold = 0.92;  // Seuil de correspondance
  private readonly maxEntries = 10_000;          // Limite d'entrÃ©es
  private readonly ttlMs = 7 * 24 * 60 * 60 * 1000; // TTL : 7 jours

  constructor(embedder: Embedder) {
    this.embedder = embedder;
  }

  /**
   * ğŸ” Recherche une correspondance sÃ©mantique dans le cache
   *
   * @param query - La requÃªte Ã  chercher
   * @returns La meilleure correspondance ou null
   */
  async get(query: string): Promise<CacheResult | null> {
    // 1ï¸âƒ£ Calculer l'embedding de la requÃªte
    const queryEmbedding = await this.embedder.embed(query);

    // 2ï¸âƒ£ Chercher la meilleure correspondance
    let bestMatch: CacheEntry | null = null;
    let bestSimilarity = 0;

    for (const entry of this.entries.values()) {
      // â° VÃ©rifier le TTL
      if (Date.now() - entry.createdAt.getTime() > this.ttlMs) {
        this.entries.delete(entry.id);
        continue;
      }

      // ğŸ“ Calculer la similaritÃ©
      const similarity = this.cosineSimilarity(
        queryEmbedding,
        entry.queryEmbedding
      );

      if (similarity > bestSimilarity &&
          similarity >= this.similarityThreshold) {
        bestSimilarity = similarity;
        bestMatch = entry;
      }
    }

    // 3ï¸âƒ£ Retourner le rÃ©sultat
    if (bestMatch) {
      // ğŸ“Š Mettre Ã  jour les stats
      bestMatch.accessCount++;
      bestMatch.lastAccess = new Date();

      return {
        response: bestMatch.response,
        similarity: bestSimilarity,
        originalQuery: bestMatch.query,
        metadata: bestMatch.metadata
      };
    }

    return null;
  }

  /**
   * ğŸ’¾ Ajoute une nouvelle entrÃ©e au cache
   *
   * @param query - La requÃªte
   * @param response - La rÃ©ponse Ã  cacher
   * @param metadata - MÃ©tadonnÃ©es (modÃ¨le, tokens)
   */
  async set(
    query: string,
    response: string,
    metadata: CacheEntry['metadata']
  ): Promise<void> {
    // ğŸ§¹ VÃ©rifier la limite
    if (this.entries.size >= this.maxEntries) {
      this.evictLeastValuable();
    }

    // ğŸ§® Calculer l'embedding
    const queryEmbedding = await this.embedder.embed(query);

    // ğŸ“¦ CrÃ©er l'entrÃ©e
    const entry: CacheEntry = {
      id: createHash('sha256')
        .update(query + Date.now())
        .digest('hex')
        .slice(0, 16),
      query,
      queryEmbedding,
      response,
      createdAt: new Date(),
      accessCount: 0,
      lastAccess: new Date(),
      metadata
    };

    this.entries.set(entry.id, entry);
  }

  /**
   * ğŸ“ Calcule la similaritÃ© cosine entre deux vecteurs
   */
  private cosineSimilarity(a: number[], b: number[]): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  /**
   * ğŸ§¹ Ã‰viction intelligente - LRU pondÃ©rÃ© par popularitÃ©
   *
   * Au lieu d'un simple LRU, on calcule un score qui combine :
   * - La rÃ©cence (quand a-t-elle Ã©tÃ© accÃ©dÃ©e ?)
   * - La frÃ©quence (combien de fois ?)
   */
  private evictLeastValuable(): void {
    let victim: CacheEntry | null = null;
    let lowestScore = Infinity;

    for (const entry of this.entries.values()) {
      // ğŸ“Š Score = accÃ¨s par heure depuis crÃ©ation
      const ageHours = (Date.now() - entry.createdAt.getTime()) / 3600000;
      const score = entry.accessCount / Math.max(ageHours, 1);

      if (score < lowestScore) {
        lowestScore = score;
        victim = entry;
      }
    }

    if (victim) {
      this.entries.delete(victim.id);
    }
  }

  /**
   * ğŸ’¾ Persiste le cache sur disque
   */
  async save(path: string): Promise<void> {
    const data = Array.from(this.entries.values());
    await fs.writeFile(path, JSON.stringify(data, null, 2));
  }

  /**
   * ğŸ“‚ Charge le cache depuis le disque
   */
  async load(path: string): Promise<void> {
    try {
      const raw = await fs.readFile(path, 'utf-8');
      const data = JSON.parse(raw);

      for (const entry of data) {
        entry.createdAt = new Date(entry.createdAt);
        entry.lastAccess = new Date(entry.lastAccess);
        this.entries.set(entry.id, entry);
      }
    } catch {
      // Fichier inexistant ou corrompu â€” on commence vide
    }
  }

  /**
   * ğŸ“Š Retourne les statistiques du cache
   */
  getStats(): CacheStats {
    let totalAccess = 0;
    let oldestEntry: Date | null = null;

    for (const entry of this.entries.values()) {
      totalAccess += entry.accessCount;
      if (!oldestEntry || entry.createdAt < oldestEntry) {
        oldestEntry = entry.createdAt;
      }
    }

    return {
      entries: this.entries.size,
      totalAccesses: totalAccess,
      avgAccessesPerEntry: this.entries.size > 0
        ? totalAccess / this.entries.size
        : 0,
      oldestEntry,
      estimatedSavings: totalAccess * 0.03 // $0.03 par requÃªte Ã©conomisÃ©e
    };
  }
}
```

### 12.2.3 ğŸ”Œ IntÃ©gration avec l'Agent

```typescript
// src/agent/grok-agent.ts
export class CodeBuddyAgent {
  private semanticCache: SemanticCache;
  private cacheHits = 0;
  private cacheMisses = 0;

  async chat(message: string): Promise<string> {
    // 1ï¸âƒ£ VÃ©rifier le cache sÃ©mantique
    const cached = await this.semanticCache.get(message);

    if (cached) {
      this.cacheHits++;
      console.log(
        `âœ… [Cache HIT] Similarity: ${(cached.similarity * 100).toFixed(1)}%`
      );
      console.log(`   Original: "${cached.originalQuery.slice(0, 50)}..."`);
      return cached.response;
    }

    this.cacheMisses++;
    console.log(`âŒ [Cache MISS] Calling LLM...`);

    // 2ï¸âƒ£ Appeler le LLM
    const response = await this.client.chat(this.buildMessages(message));

    // 3ï¸âƒ£ Cacher la rÃ©ponse pour les futures requÃªtes similaires
    await this.semanticCache.set(message, response.content, {
      model: this.currentModel,
      tokens: response.usage.totalTokens
    });

    return response.content;
  }

  /**
   * ğŸ“Š Retourne le taux de hits du cache
   */
  getCacheHitRate(): number {
    const total = this.cacheHits + this.cacheMisses;
    return total > 0 ? this.cacheHits / total : 0;
  }
}
```

### 12.2.4 ğŸ“Š Comparaison des Approches

| Approche | Hit Rate | Faux Positifs | ComplexitÃ© | CoÃ»t Embedding |
|----------|:--------:|:-------------:|:----------:|:--------------:|
| **Cache exact** | ~20% | 0% | O(1) | Aucun |
| **Cache normalisÃ©** | ~35% | ~1% | O(1) | Aucun |
| **Cache sÃ©mantique** | ~68% | ~3% | O(n) | $0.0001/req |
| **Cache sÃ©m. + LSH** | ~65% | ~4% | O(1) | $0.0001/req |

> ğŸ’¡ **LSH (Locality-Sensitive Hashing)** : Technique pour accÃ©lÃ©rer la recherche de voisins proches. Au lieu de comparer avec tous les vecteurs (O(n)), on hache les vecteurs de maniÃ¨re Ã  ce que les vecteurs similaires aient le mÃªme hash (O(1)).

---

## 12.3 ğŸ”§ Tool Result Cache

Les outils aussi peuvent Ãªtre cachÃ©s. Certains retournent des rÃ©sultats stables â€” lire un fichier qui n'a pas changÃ© retourne toujours le mÃªme contenu.

### 12.3.1 ğŸ“Š Classification des Outils

| Outil | IcÃ´ne | StabilitÃ© | Cacheable | StratÃ©gie |
|-------|:-----:|-----------|:---------:|-----------|
| `read_file` | ğŸ“„ | Stable jusqu'Ã  modification | âœ… | TTL + invalidation |
| `list_directory` | ğŸ“ | Change rarement | âœ… | TTL court (2 min) |
| `search_content` | ğŸ” | Stable par session | âœ… | TTL moyen (15 min) |
| `git_status` | ğŸ“Š | Change souvent | âŒ | Pas de cache |
| `bash` (pure) | ğŸ’» | DÃ©terministe | âš ï¸ | DÃ©pend de la commande |
| `bash` (side effects) | âš ï¸ | ImprÃ©visible | âŒ | Jamais |

### 12.3.2 ğŸ”§ ImplÃ©mentation

```typescript
// src/performance/tool-cache.ts
import { LRUCache } from 'lru-cache';

/**
 * ğŸ“¦ EntrÃ©e du cache d'outil
 */
interface ToolCacheEntry {
  key: string;              // ğŸ”‘ ClÃ© unique (outil + args)
  result: ToolResult;       // ğŸ“¤ RÃ©sultat de l'exÃ©cution
  timestamp: Date;          // â° Moment du cache
  ttl: number;              // â³ DurÃ©e de vie en ms
  invalidators: string[];   // ğŸ¯ Chemins qui invalident cette entrÃ©e
}

/**
 * âš™ï¸ Configuration par outil
 */
interface ToolCacheConfig {
  enabled: boolean;
  ttl: number;
  keyGenerator: (args: Record<string, unknown>) => string;
  invalidators: (args: Record<string, unknown>) => string[];
}

/**
 * ğŸ”§ ToolCache - Cache intelligent pour les rÃ©sultats d'outils
 *
 * Chaque outil a sa propre stratÃ©gie de caching :
 * - read_file : cache long, invalidÃ© par Ã©criture
 * - list_directory : cache court, invalidÃ© par changement
 * - search : cache moyen, invalidÃ© par toute Ã©criture
 */
export class ToolCache {
  private cache: LRUCache<string, ToolCacheEntry>;
  private readonly defaultTTL = 5 * 60 * 1000; // 5 minutes

  // ğŸ“‹ Configuration par outil
  private readonly toolConfig: Record<string, ToolCacheConfig> = {
    'read_file': {
      enabled: true,
      ttl: 10 * 60 * 1000, // 10 minutes
      keyGenerator: (args) => `read:${args.path}`,
      invalidators: (args) => [args.path as string]
    },
    'list_directory': {
      enabled: true,
      ttl: 2 * 60 * 1000, // 2 minutes
      keyGenerator: (args) => `ls:${args.path}`,
      invalidators: (args) => [args.path as string]
    },
    'search_content': {
      enabled: true,
      ttl: 15 * 60 * 1000, // 15 minutes
      keyGenerator: (args) => `search:${args.pattern}:${args.path || '*'}`,
      invalidators: () => [] // InvalidÃ© globalement
    },
    'git_status': {
      enabled: false, // Trop volatil
      ttl: 0,
      keyGenerator: () => 'git:status',
      invalidators: () => []
    },
    'bash': {
      enabled: false, // Side effects potentiels
      ttl: 0,
      keyGenerator: () => '',
      invalidators: () => []
    }
  };

  constructor() {
    this.cache = new LRUCache<string, ToolCacheEntry>({
      max: 1000,
      ttl: this.defaultTTL
    });
  }

  /**
   * ğŸ” Cherche un rÃ©sultat dans le cache
   */
  async get(toolName: string, args: Record<string, unknown>): Promise<ToolResult | null> {
    const config = this.toolConfig[toolName];
    if (!config?.enabled) return null;

    const key = config.keyGenerator(args);
    const entry = this.cache.get(key);

    if (entry) {
      // â° VÃ©rifier le TTL
      const age = Date.now() - entry.timestamp.getTime();
      if (age < entry.ttl) {
        console.log(`ğŸ”§ [Tool Cache HIT] ${toolName}: ${key}`);
        return entry.result;
      }
    }

    return null;
  }

  /**
   * ğŸ’¾ Stocke un rÃ©sultat dans le cache
   */
  async set(
    toolName: string,
    args: Record<string, unknown>,
    result: ToolResult
  ): Promise<void> {
    const config = this.toolConfig[toolName];
    if (!config?.enabled) return;
    if (!result.success) return; // âŒ Ne pas cacher les erreurs

    const key = config.keyGenerator(args);
    const invalidators = config.invalidators(args);

    this.cache.set(key, {
      key,
      result,
      timestamp: new Date(),
      ttl: config.ttl ?? this.defaultTTL,
      invalidators
    });
  }

  /**
   * ğŸ—‘ï¸ Invalide les entrÃ©es liÃ©es Ã  un chemin
   */
  invalidate(path: string): void {
    let invalidated = 0;

    for (const [key, entry] of this.cache.entries()) {
      const shouldInvalidate = entry.invalidators.some(inv =>
        path.startsWith(inv) || inv.startsWith(path)
      );

      if (shouldInvalidate) {
        this.cache.delete(key);
        invalidated++;
      }
    }

    if (invalidated > 0) {
      console.log(`ğŸ—‘ï¸ [Tool Cache] Invalidated ${invalidated} entries for: ${path}`);
    }
  }

  /**
   * ğŸ§¹ Invalide tout le cache
   */
  invalidateAll(): void {
    const size = this.cache.size;
    this.cache.clear();
    console.log(`ğŸ§¹ [Tool Cache] Cleared all ${size} entries`);
  }
}
```

### 12.3.3 ğŸ”„ Invalidation Intelligente

L'invalidation est la partie la plus dÃ©licate du caching. Un cache qui sert des donnÃ©es pÃ©rimÃ©es est pire que pas de cache du tout.

```typescript
// src/performance/cache-invalidator.ts
import { watch, FSWatcher } from 'fs';
import { EventEmitter } from 'events';

/**
 * ğŸ‘ï¸ FileWatcher - Surveille les modifications de fichiers
 * et invalide le cache automatiquement
 */
export class CacheInvalidator extends EventEmitter {
  private watcher: FSWatcher | null = null;
  private toolCache: ToolCache;
  private debounceTimers: Map<string, NodeJS.Timeout> = new Map();

  constructor(toolCache: ToolCache) {
    super();
    this.toolCache = toolCache;
  }

  /**
   * ğŸ‘ï¸ DÃ©marre la surveillance d'un rÃ©pertoire
   */
  start(directory: string): void {
    this.watcher = watch(directory, { recursive: true });

    this.watcher.on('change', (eventType, filename) => {
      if (eventType === 'change' || eventType === 'rename') {
        const fullPath = path.join(directory, filename as string);

        // ğŸ”„ Debounce pour Ã©viter les invalidations multiples
        this.debounce(fullPath, () => {
          this.toolCache.invalidate(fullPath);
          this.emit('invalidated', fullPath);
        });
      }
    });

    console.log(`ğŸ‘ï¸ Watching ${directory} for changes`);
  }

  private debounce(key: string, fn: () => void, ms = 100): void {
    const existing = this.debounceTimers.get(key);
    if (existing) clearTimeout(existing);

    this.debounceTimers.set(key, setTimeout(() => {
      fn();
      this.debounceTimers.delete(key);
    }, ms));
  }

  stop(): void {
    this.watcher?.close();
    this.debounceTimers.forEach(t => clearTimeout(t));
    this.debounceTimers.clear();
  }
}

/**
 * ğŸ”— Hook d'invalidation post-outil
 * Certains outils modifient le systÃ¨me de fichiers â€” il faut
 * invalider le cache aprÃ¨s leur exÃ©cution.
 */
const INVALIDATING_TOOLS = [
  'write_file',
  'edit_file',
  'delete_file',
  'bash'
];

export function afterToolExecution(
  toolName: string,
  args: Record<string, unknown>,
  result: ToolResult,
  toolCache: ToolCache
): void {
  if (!INVALIDATING_TOOLS.includes(toolName)) return;
  if (!result.success) return;

  if (toolName === 'bash') {
    // âš ï¸ On ne sait pas ce que la commande a fait
    // Invalidation totale par sÃ©curitÃ©
    toolCache.invalidateAll();
  } else if (args.path) {
    // ğŸ¯ Invalidation ciblÃ©e
    toolCache.invalidate(args.path as string);
  }
}
```

---

## 12.4 âš¡ PrÃ©-calcul et Warming

PlutÃ´t que d'attendre les requÃªtes, on peut **anticiper** les besoins et prÃ©calculer les donnÃ©es frÃ©quemment utilisÃ©es.

### 12.4.1 ğŸš€ PrÃ©-chargement du Contexte

```typescript
// src/performance/context-preloader.ts

/**
 * ğŸš€ ContextPreloader - PrÃ©charge le contexte au dÃ©marrage
 *
 * StratÃ©gie : identifier les fichiers "importants" et les
 * prÃ©-indexer avant que l'utilisateur ne les demande.
 */
export class ContextPreloader {
  private embedder: Embedder;
  private ragRetriever: CodebaseRetriever;
  private toolCache: ToolCache;

  // ğŸ“‹ Patterns de fichiers importants (par ordre de prioritÃ©)
  private readonly importantPatterns = [
    '**/package.json',        // ğŸ“¦ DÃ©pendances
    '**/README.md',           // ğŸ“– Documentation
    '**/src/index.{ts,js}',   // ğŸšª Point d'entrÃ©e
    '**/src/types/**',        // ğŸ“ Types partagÃ©s
    '**/.env.example',        // âš™ï¸ Configuration
    '**/tsconfig.json',       // ğŸ”§ Config TypeScript
    '**/Dockerfile',          // ğŸ³ Conteneurisation
  ];

  async preload(projectRoot: string): Promise<PreloadResult> {
    console.log('ğŸš€ Preloading context...');
    const startTime = Date.now();
    let filesProcessed = 0;

    // 1ï¸âƒ£ PrÃ©-calculer les embeddings des fichiers importants
    for (const pattern of this.importantPatterns) {
      const files = await glob(pattern, { cwd: projectRoot });

      for (const file of files) {
        await this.ragRetriever.ensureIndexed(file);
        filesProcessed++;
      }
    }

    // 2ï¸âƒ£ PrÃ©-charger les mÃ©tadonnÃ©es des dÃ©pendances
    await this.preloadDependencies(projectRoot);

    // 3ï¸âƒ£ PrÃ©-cacher les structures de rÃ©pertoires frÃ©quentes
    await this.precacheDirectories(projectRoot);

    const duration = Date.now() - startTime;
    console.log(`âœ… Context preloaded: ${filesProcessed} files in ${duration}ms`);

    return {
      filesProcessed,
      duration,
      cacheWarmth: this.calculateWarmth()
    };
  }

  private async preloadDependencies(projectRoot: string): Promise<void> {
    const packagePath = path.join(projectRoot, 'package.json');

    try {
      const pkg = JSON.parse(await fs.readFile(packagePath, 'utf-8'));
      const deps = Object.keys(pkg.dependencies || {}).slice(0, 10);

      console.log(`ğŸ“¦ Preloading info for ${deps.length} dependencies...`);

      // PrÃ©-fetcher les infos des dÃ©pendances principales
      for (const dep of deps) {
        await this.fetchDependencyInfo(dep);
      }
    } catch {
      // Pas de package.json â€” on continue
    }
  }

  private async precacheDirectories(projectRoot: string): Promise<void> {
    const commonDirs = ['src', 'lib', 'tests', 'docs'];

    for (const dir of commonDirs) {
      const fullPath = path.join(projectRoot, dir);
      if (await exists(fullPath)) {
        // Simuler un list_directory pour le mettre en cache
        const entries = await fs.readdir(fullPath, { withFileTypes: true });
        await this.toolCache.set('list_directory', { path: fullPath }, {
          success: true,
          output: entries.map(e => ({
            name: e.name,
            type: e.isDirectory() ? 'directory' : 'file'
          }))
        });
      }
    }
  }

  private calculateWarmth(): number {
    // Ratio entrÃ©es en cache / entrÃ©es attendues
    const stats = this.toolCache.getStats();
    return Math.min(stats.size / 100, 1.0);
  }
}
```

### 12.4.2 ğŸ“‹ Cache de Templates

Les prompts suivent souvent des patterns rÃ©pÃ©titifs. En prÃ©-compilant les templates, on Ã©conomise du traitement.

```typescript
// src/performance/template-cache.ts

/**
 * ğŸ“‹ Template compilÃ© et prÃªt Ã  l'emploi
 */
interface CompiledTemplate {
  name: string;
  template: string;
  variables: string[];
  render: (values: Record<string, string>) => string;
}

/**
 * ğŸ“‹ PromptTemplateCache - PrÃ©-compile les templates de prompts
 *
 * Exemple :
 *   template: "Explain {{code}} focusing on {{aspect}}"
 *   values: { code: "...", aspect: "performance" }
 *   result: "Explain ... focusing on performance"
 */
export class PromptTemplateCache {
  private templates: Map<string, CompiledTemplate> = new Map();

  constructor() {
    this.precompile();
  }

  private precompile(): void {
    const commonTemplates: Record<string, string> = {
      'code_explanation': `
Explain the following {{language}} code:

\`\`\`{{language}}
{{code}}
\`\`\`

Focus on: {{focus}}
Explain step by step what it does.
      `.trim(),

      'bug_fix': `
Fix this bug in {{language}} code:

**Error:** {{error}}

**Code:**
\`\`\`{{language}}
{{code}}
\`\`\`

**Expected behavior:** {{expected_behavior}}

Provide the corrected code with an explanation.
      `.trim(),

      'refactor': `
Refactor this {{language}} code to improve {{aspect}}:

\`\`\`{{language}}
{{code}}
\`\`\`

**Constraints:**
{{constraints}}

Show the refactored version with explanations.
      `.trim(),

      'test_generation': `
Generate tests for this {{language}} code using {{framework}}:

\`\`\`{{language}}
{{code}}
\`\`\`

Include tests for: {{scenarios}}
      `.trim()
    };

    for (const [name, template] of Object.entries(commonTemplates)) {
      this.templates.set(name, this.compile(name, template));
    }

    console.log(`ğŸ“‹ Precompiled ${this.templates.size} prompt templates`);
  }

  private compile(name: string, template: string): CompiledTemplate {
    // Extraire les variables {{var}}
    const matches = template.match(/\{\{(\w+)\}\}/g) || [];
    const variables = [...new Set(matches.map(v => v.slice(2, -2)))];

    return {
      name,
      template,
      variables,
      render: (values: Record<string, string>) => {
        let result = template;
        for (const [key, value] of Object.entries(values)) {
          result = result.replace(
            new RegExp(`\\{\\{${key}\\}\\}`, 'g'),
            value
          );
        }
        return result;
      }
    };
  }

  render(templateName: string, values: Record<string, string>): string {
    const template = this.templates.get(templateName);

    if (!template) {
      throw new Error(`Template not found: ${templateName}`);
    }

    // VÃ©rifier que toutes les variables sont fournies
    const missing = template.variables.filter(v => !(v in values));
    if (missing.length > 0) {
      throw new Error(
        `Missing template variables: ${missing.join(', ')}`
      );
    }

    return template.render(values);
  }

  list(): string[] {
    return Array.from(this.templates.keys());
  }
}
```

---

## 12.5 ğŸ“Š MÃ©triques et Monitoring

Sans mÃ©triques, on optimise Ã  l'aveugle. Un bon dashboard rÃ©vÃ¨le les opportunitÃ©s d'amÃ©lioration.

### 12.5.1 ğŸ›ï¸ Dashboard d'Optimisation

![Dashboard d'Optimisation](images/optimization-dashboard.svg)

### 12.5.2 ğŸ“ˆ ImplÃ©mentation des MÃ©triques

```typescript
// src/performance/optimization-metrics.ts

/**
 * ğŸ“Š Structure des mÃ©triques d'optimisation
 */
interface OptimizationMetrics {
  // ğŸ”® Cache sÃ©mantique
  semanticCache: {
    hits: number;
    misses: number;
    hitRate: number;
    avgSimilarity: number;
    entries: number;
    estimatedSavings: number;
  };

  // ğŸ”§ Cache outils
  toolCache: {
    hits: number;
    misses: number;
    hitRate: number;
    invalidations: number;
    entries: number;
    memoryMB: number;
  };

  // ğŸ’° CoÃ»ts
  cost: {
    totalRequests: number;
    cachedRequests: number;
    apiCalls: number;
    estimatedCost: number;  // Sans cache
    actualCost: number;     // Avec cache
    savings: number;
    savingsPercent: number;
  };

  // â±ï¸ Performance
  performance: {
    avgCacheLookupMs: number;
    avgEmbeddingMs: number;
    avgLlmCallMs: number;
    avgCacheHitMs: number;
  };
}

/**
 * ğŸ“Š MetricsCollector - Collecte et agrÃ¨ge les mÃ©triques
 */
export class MetricsCollector {
  private semanticHits = 0;
  private semanticMisses = 0;
  private toolHits = 0;
  private toolMisses = 0;
  private toolInvalidations = 0;
  private similarities: number[] = [];
  private timings: Record<string, number[]> = {
    cacheLookup: [],
    embedding: [],
    llmCall: [],
    cacheHit: []
  };

  recordSemanticHit(similarity: number): void {
    this.semanticHits++;
    this.similarities.push(similarity);
  }

  recordSemanticMiss(): void {
    this.semanticMisses++;
  }

  recordToolHit(): void {
    this.toolHits++;
  }

  recordToolMiss(): void {
    this.toolMisses++;
  }

  recordToolInvalidation(): void {
    this.toolInvalidations++;
  }

  recordTiming(type: keyof typeof this.timings, ms: number): void {
    this.timings[type].push(ms);
    // Garder seulement les 1000 derniÃ¨res mesures
    if (this.timings[type].length > 1000) {
      this.timings[type].shift();
    }
  }

  getMetrics(
    semanticCache: SemanticCache,
    toolCache: ToolCache
  ): OptimizationMetrics {
    const semanticTotal = this.semanticHits + this.semanticMisses;
    const toolTotal = this.toolHits + this.toolMisses;

    const avgSimilarity = this.similarities.length > 0
      ? this.similarities.reduce((a, b) => a + b, 0) / this.similarities.length
      : 0;

    const estimatedCost = (this.semanticHits + this.semanticMisses) * 0.05;
    const actualCost = this.semanticMisses * 0.05;
    const savings = estimatedCost - actualCost;

    return {
      semanticCache: {
        hits: this.semanticHits,
        misses: this.semanticMisses,
        hitRate: semanticTotal > 0 ? this.semanticHits / semanticTotal : 0,
        avgSimilarity,
        entries: semanticCache.getStats().entries,
        estimatedSavings: this.semanticHits * 0.03
      },
      toolCache: {
        hits: this.toolHits,
        misses: this.toolMisses,
        hitRate: toolTotal > 0 ? this.toolHits / toolTotal : 0,
        invalidations: this.toolInvalidations,
        entries: toolCache.getStats().size,
        memoryMB: toolCache.getStats().memoryBytes / (1024 * 1024)
      },
      cost: {
        totalRequests: semanticTotal,
        cachedRequests: this.semanticHits,
        apiCalls: this.semanticMisses,
        estimatedCost,
        actualCost,
        savings,
        savingsPercent: estimatedCost > 0 ? (savings / estimatedCost) * 100 : 0
      },
      performance: {
        avgCacheLookupMs: this.avgTiming('cacheLookup'),
        avgEmbeddingMs: this.avgTiming('embedding'),
        avgLlmCallMs: this.avgTiming('llmCall'),
        avgCacheHitMs: this.avgTiming('cacheHit')
      }
    };
  }

  private avgTiming(type: string): number {
    const values = this.timings[type];
    if (values.length === 0) return 0;
    return values.reduce((a, b) => a + b, 0) / values.length;
  }
}
```

### 12.5.3 âš ï¸ Alertes d'Optimisation

```typescript
// src/performance/optimization-alerts.ts

interface Alert {
  level: 'info' | 'warning' | 'error';
  code: string;
  message: string;
  suggestion: string;
}

/**
 * âš ï¸ VÃ©rifie la santÃ© des optimisations et gÃ©nÃ¨re des alertes
 */
export function checkOptimizationHealth(
  metrics: OptimizationMetrics
): Alert[] {
  const alerts: Alert[] = [];

  // ğŸ“‰ Cache hit rate trop bas
  if (metrics.semanticCache.hitRate < 0.3) {
    alerts.push({
      level: 'warning',
      code: 'LOW_HIT_RATE',
      message: `Semantic cache hit rate at ${(metrics.semanticCache.hitRate * 100).toFixed(1)}%`,
      suggestion: 'Consider lowering similarity threshold (currently 0.92)'
    });
  }

  // ğŸ”„ Trop d'invalidations
  if (metrics.toolCache.invalidations > metrics.toolCache.hits) {
    alerts.push({
      level: 'info',
      code: 'HIGH_INVALIDATION',
      message: `Tool cache invalidations (${metrics.toolCache.invalidations}) exceed hits`,
      suggestion: 'This workflow may not benefit from tool caching'
    });
  }

  // ğŸ’° CoÃ»t Ã©levÃ© malgrÃ© le cache
  if (metrics.cost.actualCost > 100 && metrics.cost.savingsPercent < 20) {
    alerts.push({
      level: 'warning',
      code: 'LOW_SAVINGS',
      message: `High costs ($${metrics.cost.actualCost.toFixed(2)}) with only ${metrics.cost.savingsPercent.toFixed(1)}% savings`,
      suggestion: 'Review caching strategy or query patterns'
    });
  }

  // â±ï¸ Cache lookup trop lent
  if (metrics.performance.avgCacheLookupMs > 100) {
    alerts.push({
      level: 'warning',
      code: 'SLOW_CACHE',
      message: `Cache lookup averaging ${metrics.performance.avgCacheLookupMs.toFixed(1)}ms`,
      suggestion: 'Consider implementing LSH for O(1) lookups'
    });
  }

  // ğŸ“Š SimilaritÃ© moyenne basse
  if (metrics.semanticCache.avgSimilarity > 0 &&
      metrics.semanticCache.avgSimilarity < 0.90) {
    alerts.push({
      level: 'info',
      code: 'LOW_SIMILARITY',
      message: `Average match similarity at ${(metrics.semanticCache.avgSimilarity * 100).toFixed(1)}%`,
      suggestion: 'Matches may be less accurate than ideal'
    });
  }

  return alerts;
}
```

---

## 12.6 âœ… Bonnes Pratiques

### 12.6.1 ğŸ“‹ Matrice de DÃ©cision : Cacher ou Non ?

| Situation | Cacher ? | IcÃ´ne | Raison |
|-----------|:--------:|:-----:|--------|
| Questions gÃ©nÃ©rales frÃ©quentes | âœ… Oui | ğŸ”® | ROI Ã©levÃ© |
| RÃ©ponses personnalisÃ©es | âŒ Non | ğŸ¯ | Contexte diffÃ©rent |
| Outils dÃ©terministes | âœ… Oui | ğŸ”§ | RÃ©sultat stable |
| Outils avec side effects | âŒ Non | âš ï¸ | Comportement imprÃ©visible |
| Session longue (> 1h) | âœ… TTL court | â³ | Contexte Ã©volue |
| Multi-utilisateurs | âš ï¸ Attention | ğŸ‘¥ | Isolation nÃ©cessaire |
| DonnÃ©es sensibles | âŒ Non | ğŸ”’ | Risque de fuite |

### 12.6.2 ğŸšï¸ Tuning du Seuil de SimilaritÃ©

| Seuil | Hit Rate | Faux Positifs | Recommandation |
|:-----:|:--------:|:-------------:|----------------|
| 0.99 | ~25% | ~0% | ğŸ”’ Ultra-conservateur |
| 0.95 | ~50% | ~1% | âœ… **Production recommandÃ©** |
| 0.92 | ~65% | ~3% | âš–ï¸ Ã‰quilibrÃ© (dÃ©faut) |
| 0.90 | ~72% | ~5% | ğŸš€ Agressif |
| 0.85 | ~80% | ~12% | âš ï¸ Risque qualitÃ© |

> ğŸ’¡ **Conseil** : Commencez Ã  0.92, mesurez pendant une semaine, puis ajustez selon les faux positifs observÃ©s.

### 12.6.3 ğŸ’¾ Gestion de la MÃ©moire

```typescript
// src/performance/memory-manager.ts

/**
 * ğŸ’¾ Gestionnaire de mÃ©moire pour les caches
 */
export class CacheMemoryManager {
  private readonly MAX_MEMORY = 100 * 1024 * 1024; // 100 MB

  /**
   * ğŸ“ Estime la taille d'une entrÃ©e en mÃ©moire
   */
  estimateEntrySize(entry: CacheEntry): number {
    return (
      entry.query.length * 2 +           // UTF-16
      entry.queryEmbedding.length * 8 +  // Float64
      entry.response.length * 2 +         // UTF-16
      200                                 // Overhead objet
    );
  }

  /**
   * ğŸ“Š Calcule la mÃ©moire totale utilisÃ©e
   */
  calculateTotalMemory(entries: CacheEntry[]): number {
    return entries.reduce(
      (total, entry) => total + this.estimateEntrySize(entry),
      0
    );
  }

  /**
   * ğŸ§¹ Enforce la limite de mÃ©moire
   */
  enforceLimit(cache: SemanticCache): number {
    let totalSize = 0;
    let evicted = 0;
    const entries = Array.from(cache.entries());

    // Calculer la taille totale
    for (const entry of entries) {
      totalSize += this.estimateEntrySize(entry);
    }

    // Ã‰viction si nÃ©cessaire
    while (totalSize > this.MAX_MEMORY && entries.length > 0) {
      const oldest = this.findLeastValuable(entries);
      totalSize -= this.estimateEntrySize(oldest);
      cache.delete(oldest.id);
      evicted++;
    }

    if (evicted > 0) {
      console.log(
        `ğŸ§¹ Memory enforcement: evicted ${evicted} entries, ` +
        `freed ${(evicted * 50 / 1024).toFixed(1)} KB`
      );
    }

    return evicted;
  }

  private findLeastValuable(entries: CacheEntry[]): CacheEntry {
    return entries.reduce((min, entry) => {
      const minScore = this.valueScore(min);
      const entryScore = this.valueScore(entry);
      return entryScore < minScore ? entry : min;
    });
  }

  private valueScore(entry: CacheEntry): number {
    const ageHours = (Date.now() - entry.lastAccess.getTime()) / 3600000;
    return entry.accessCount / Math.max(ageHours, 0.1);
  }
}
```

### 12.6.4 ğŸ“Š Tableau RÃ©capitulatif des RÃ©sultats

| MÃ©trique | Sans Optimisation | Avec Optimisation | AmÃ©lioration |
|----------|------------------:|------------------:|-------------:|
| RequÃªtes API/jour | 10,000 | 3,200 | -68% |
| CoÃ»t/jour | $500 | $170 | -66% |
| Latence moyenne | 1,200ms | 420ms | -65% |
| Latence P99 | 3,500ms | 1,800ms | -49% |

---

## âš ï¸ 12.7 Limites et Risques

### ğŸš§ Limites Techniques

| Limite | Description | Mitigation |
|--------|-------------|------------|
| **Faux positifs du cache** | RÃ©ponse similaire mais incorrecte pour le contexte | Seuil de similaritÃ© Ã©levÃ© (>0.92) |
| **DÃ©rive temporelle** | Cache obsolÃ¨te si le contexte Ã©volue | TTL appropriÃ©, invalidation proactive |
| **CoÃ»t des embeddings** | Chaque lookup = 1 embedding | Cache des embeddings de requÃªtes |
| **MÃ©moire RAM** | Cache volumineux = pression mÃ©moire | LRU avec limite stricte |
| **Cold start** | Aucun bÃ©nÃ©fice Ã  la premiÃ¨re session | PrÃ©-chauffage des requÃªtes frÃ©quentes |

### âš ï¸ Risques OpÃ©rationnels

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|:-----------:|:------:|------------|
| **RÃ©ponses pÃ©rimÃ©es** | Moyenne | Moyen | Invalidation sur changement de fichier |
| **Cache poisoning** | Faible | Ã‰levÃ© | Validation des entrÃ©es, isolation |
| **Consommation mÃ©moire** | Moyenne | Moyen | Monitoring, Ã©viction automatique |
| **Sur-optimisation** | Moyenne | Moyen | Mesurer avant d'optimiser |
| **Fuite d'info entre sessions** | Faible | Ã‰levÃ© | Isolation par utilisateur/projet |

### ğŸ“Š Quand NE PAS Utiliser le Cache

| Situation | Raison |
|-----------|--------|
| Questions personnalisÃ©es | Le contexte change la rÃ©ponse |
| Analyse de code live | Les fichiers changent frÃ©quemment |
| Sessions multi-utilisateurs | Risque de fuite entre contextes |
| DonnÃ©es sensibles | Le cache persiste sur disque |
| PremiÃ¨re utilisation | Pas de historique Ã  exploiter |

### ğŸ’¡ Recommandations

> ğŸ’¡ **Astuce** : Commencez avec un seuil de similaritÃ© conservateur (0.95) et baissez progressivement en surveillant les faux positifs. Le coÃ»t d'une mauvaise rÃ©ponse dÃ©passe largement les Ã©conomies d'un cache agressif.

---

## ğŸ“ Points ClÃ©s

| Concept | IcÃ´ne | Description | Impact |
|---------|:-----:|-------------|--------|
| **Redondance** | ğŸ’¸ | 68% des requÃªtes sont similaires | OpportunitÃ© majeure |
| **Semantic Cache** | ğŸ”® | SimilaritÃ© cosine > seuil | 66% Ã©conomies API |
| **Tool Cache** | ğŸ”§ | LRU + TTL + invalidation | Latence rÃ©duite |
| **PrÃ©-calcul** | âš¡ | Embeddings, templates, contexte | DÃ©marrage rapide |
| **Monitoring** | ğŸ“Š | Dashboard en temps rÃ©el | AmÃ©lioration continue |

---

## ğŸ‹ï¸ Exercices

### Exercice 1 : ğŸšï¸ Calibration du Seuil
Testez diffÃ©rents seuils de similaritÃ© (0.85, 0.90, 0.95, 0.99) sur votre workload typique. Mesurez :
- Le hit rate
- Le nombre de faux positifs (rÃ©ponses incorrectes)
- La satisfaction utilisateur

**Objectif** : Trouver le seuil optimal pour votre cas d'usage.

### Exercice 2 : ğŸ”„ Invalidation AvancÃ©e
ImplÃ©mentez un systÃ¨me d'invalidation basÃ© sur :
- Les timestamps de fichiers
- Les dÃ©pendances entre fichiers (si A importe B, invalider A quand B change)
- Les Ã©vÃ©nements Git (commits, branches)

### Exercice 3 : ğŸ“Š Dashboard Temps RÃ©el
CrÃ©ez un dashboard TUI (Text User Interface) avec blessed ou ink qui affiche :
- Hit rates en temps rÃ©el
- Ã‰conomies cumulÃ©es
- Top 10 des requÃªtes les plus cachÃ©es
- Alertes actives

### Exercice 4 : ğŸ§ª A/B Testing
Comparez deux stratÃ©gies de caching sur une semaine :
- Groupe A : Cache sÃ©mantique seul
- Groupe B : Cache sÃ©mantique + cache d'outils

Mesurez : coÃ»ts, latence, qualitÃ© des rÃ©ponses.

---

## ğŸ“š RÃ©fÃ©rences

| Source | Description | Lien |
|--------|-------------|------|
| **GPTCache** | Semantic caching library for LLMs | [GitHub](https://github.com/zilliztech/GPTCache) |
| **Cosine Similarity** | Mesure de similaritÃ© vectorielle | [Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity) |
| **LSH** | Locality-Sensitive Hashing | [Stanford](https://cs.stanford.edu/~jtyler/lsh.pdf) |
| **LRU Cache** | Least Recently Used Ã©viction | [npm lru-cache](https://www.npmjs.com/package/lru-cache) |
| **Code Buddy** | `src/utils/semantic-cache.ts`, `src/performance/tool-cache.ts` | Local |

---

## ğŸŒ… Ã‰pilogue â€” La MÃ©moire de la Machine

*Une semaine plus tard. Vendredi soir, encore. Mais cette fois, Lina est dÃ©jÃ  debout, manteau sur le dos, sac Ã  l'Ã©paule.*

**Marc** *(surpris)* : "Tu pars Ã  l'heure ?"

**Lina** *(souriant)* : "Regarde."

*Elle tourne son Ã©cran vers lui. Le dashboard de mÃ©triques.*

```
Hit Rate:       68.2%
Ã‰conomies:      $347.50 cette semaine
Latence moy.:   420ms (vs 1,200ms avant)
Cache entries:  12,847
```

**Marc** : "68% de hit rate. Ton agent se *souvient*."

**Lina** : "Le plus beau ? Quand je tape 'ls', il reconnaÃ®t que c'est la mÃªme question que 'liste les fichiers' de ce matin. SimilaritÃ© 0.94."

*Elle fait dÃ©filer les logs.*

**Lina** : "Et regarde ici. Quand j'ai modifiÃ© `utils.ts` Ã  15h, le cache a automatiquement invalidÃ© toutes les entrÃ©es qui rÃ©fÃ©renÃ§aient ce fichier. ZÃ©ro donnÃ©e pÃ©rimÃ©e."

**Marc** : "Ã‰lÃ©gant. Tu as donnÃ© une mÃ©moire Ã  ton agent."

*Un silence. Lina hÃ©site.*

**Lina** : "Marc... Il y a quelque chose qui me tracasse quand mÃªme."

**Marc** : "Hmm ?"

**Lina** : "Le cache, c'est pour la *sortie*. On Ã©vite de recalculer les mÃªmes rÃ©ponses. Mais pour l'*entrÃ©e*..."

*Elle fait dÃ©filer jusqu'aux logs de tool calls.*

**Lina** : "Code Buddy a 41 outils. Ã€ chaque requÃªte, mon agent reÃ§oit la description de ces 41 outils. MÃªme quand la tÃ¢che est simple â€” genre lire un fichier â€” il doit traiter 41 descriptions avant de choisir."

**Marc** *(fronÃ§ant les sourcils)* : "3,000 tokens juste pour la liste des outils..."

**Lina** : "Exactement. Et j'ai lu un papier rÃ©cemment. Des chercheurs de... attend..."

*Elle cherche dans ses notes.*

**Lina** : "'Less is More: Fewer Tool Descriptions Lead to Better LLM Reasoning'. Ils ont montrÃ© que donner **moins** d'outils au modÃ¨le amÃ©liore Ã  la fois la prÃ©cision ET la vitesse."

**Marc** *(intÃ©ressÃ©)* : "Counter-intuitif. Comme JetBrains avec le contexte."

**Lina** : "MÃªme principe ! Trop de choix = paralysie de l'analyse. Si je filtre dynamiquement les outils pour ne montrer que les pertinents..."

*Elle note rapidement sur son carnet.*

**Marc** : "Tu veux implÃ©menter Ã§a ce soir ?"

**Lina** *(riant)* : "Non, je vais enfin profiter de mon vendredi. Mais lundi..."

*Elle range son carnet.*

**Lina** : "Lundi, on s'attaque aux optimisations systÃ¨me. Filtrage d'outils, routing de modÃ¨les, parallÃ©lisation..."

**Marc** : "Le trio infernal de la performance."

**Lina** : "FrugalGPT pour le routing. LLMCompiler pour la parallÃ©lisation. Et Less-is-More pour les outils."

*Elle enfile son manteau.*

**Lina** : "On a optimisÃ© la mÃ©moire. Maintenant, on optimise la rÃ©flexion elle-mÃªme."

*Elle Ã©teint son Ã©cran. La piÃ¨ce devient silencieuse, mais quelque part dans le cloud, son agent continue de servir des rÃ©ponses depuis son cache, se souvenant de chaque question dÃ©jÃ  posÃ©e.*

---

## ğŸ§­ Navigation

| PrÃ©cÃ©dent | Suivant |
|:---------:|:-------:|
| [â† Chapitre 11 : Plugins et MCP](11-plugins-mcp.md) | [Chapitre 13 : Optimisations SystÃ¨me â†’](13-optimisations-systeme.md) |

---

*Dans le prochain chapitre : Trois techniques qui ont rÃ©volutionnÃ© les agents LLM â€” FrugalGPT de Stanford, LLMCompiler de Berkeley, et le principe "Less is More" qui dÃ©fie l'intuition. PrÃ©parez-vous Ã  diviser vos coÃ»ts par trois.*
