# ğŸ§  Chapitre 1 : Comprendre les Large Language Models

---

## ğŸ¬ ScÃ¨ne d'ouverture : La Question Fondamentale

*Un mardi soir, dans un cafÃ© prÃ¨s du campus universitaire...*

Lina fixait son Ã©cran, perplexe. Elle venait de passer trois heures Ã  interagir avec ChatGPT, lui demandant d'expliquer du code, de gÃ©nÃ©rer des tests, de suggÃ©rer des refactorisations. Les rÃ©sultats Ã©taient tantÃ´t brillants, tantÃ´t absurdes.

â€” "Comment Ã§a peut Ãªtre si intelligent et si stupide Ã  la fois ?" murmura-t-elle.

Son ami Marcus, doctorant en machine learning, s'assit Ã  cÃ´tÃ© d'elle avec son cafÃ©.

â€” "Tu sais comment Ã§a fonctionne, un LLM ?"

Lina haussa les Ã©paules. "Vaguement. Des rÃ©seaux de neurones, beaucoup de donnÃ©es, quelque chose avec l'attention..."

Marcus sourit. "C'est un bon dÃ©but. Mais si tu veux vraiment construire des outils qui utilisent ces modÃ¨les, tu dois comprendre ce qu'ils sont *vraiment*. Pas la version marketing. La vraie mÃ©canique."

Il sortit un carnet et commenÃ§a Ã  dessiner. "Laisse-moi te raconter une histoire. Elle commence en 2017, dans les bureaux de Google..."

---

## ğŸ“œ 1.1 Une BrÃ¨ve Histoire des ModÃ¨les de Langage

### 1.1.1 Avant les Transformers : L'Ãˆre SÃ©quentielle (RNN)

Pour comprendre la puissance des LLMs actuels, il faut regarder en arriÃ¨re. Pendant longtemps, les RÃ©seaux de Neurones RÃ©currents (RNN) Ã©taient la norme.

Leur fonctionnement Ã©tait **sÃ©quentiel** : ils lisaient le texte mot par mot, de gauche Ã  droite, en essayant de garder en mÃ©moire le contexte prÃ©cÃ©dent.

![RNN vs Transformer](images/rnn_vs_transformer.svg)

Le problÃ¨me ? Comme le jeu du "tÃ©lÃ©phone arabe", l'information se dÃ©gradait sur la longueur. Le dÃ©but de la phrase Ã©tait souvent "oubliÃ©" avant la fin. De plus, impossible de parallÃ©liser le calcul : il fallait attendre le traitement du mot 1 pour traiter le mot 2.

### 1.1.2 âš¡ La RÃ©volution : "Attention Is All You Need" (2017)

En 2017, l'architecture **Transformer** a tout changÃ© en introduisant le mÃ©canisme d'**Attention**.

L'idÃ©e radicale : au lieu de lire sÃ©quentiellement, le modÃ¨le regarde **tous les mots en mÃªme temps**. Chaque mot peut "prÃªter attention" Ã  n'importe quel autre mot de la phrase, quelle que soit la distance qui les sÃ©pare.

| ğŸ“ˆ Comparatif | RNN (Ancien) | Transformer (Moderne) |
|:--------------|:-------------|:----------------------|
| **Traitement** | SÃ©quentiel (lent) | ParallÃ¨le (rapide sur GPU) |
| **MÃ©moire** | Courte (oublie vite) | Longue (tout le contexte) |
| **Distance** | LimitÃ©e | IllimitÃ©e (accÃ¨s direct) |

---

## ğŸ”¬ 1.2 L'Anatomie d'un Transformer

Plongeons dans le moteur. Comment le texte devient-il du sens ?

### 1.2.1 âœ‚ï¸ La Tokenisation : DÃ©couper le Langage

Le modÃ¨le ne lit pas du texte, il manipule des nombres. La premiÃ¨re Ã©tape est donc de dÃ©couper le texte en unitÃ©s appelÃ©es **tokens**.

![Processus de Tokenisation](images/tokenisation_process.svg)

Ce dÃ©coupage utilise souvent le **Byte-Pair Encoding (BPE)**, qui optimise le vocabulaire en gardant les mots frÃ©quents entiers et en dÃ©coupant les mots rares en syllabes.

#### ğŸ§ª Laboratoire : La Tokenisation en Pratique

Voici comment visualiser la tokenisation rÃ©elle avec la librairie `tiktoken` (utilisÃ©e par OpenAI) :

```typescript
import { encoding_for_model } from "tiktoken";

// On charge l'encodeur de GPT-4
const enc = encoding_for_model("gpt-4");

const text = "Le dÃ©veloppeur code bien";
const tokens = enc.encode(text);

console.log(`Texte: "${text}"`);
console.log(`Tokens (IDs): [${tokens.join(", ")}]`);
console.log(`Nombre de tokens: ${tokens.length}`);

// DÃ©codons pour voir ce que chaque token reprÃ©sente
tokens.forEach(id => {
  console.log(`${id} -> "${new TextDecoder().decode(enc.decode([id]))}"`);
});

// RÃ©sultat typique :
// "Le" -> 1 token
// " dÃ©" -> 1 token
// "velopp" -> 1 token (mot dÃ©coupÃ© !)
// "eur" -> 1 token
// ...
```

> âš ï¸ **Attention aux coÃ»ts** : Les modÃ¨les facturent au token. Un code mal formatÃ© ou verbeux coÃ»te plus cher. Notez aussi que les caractÃ¨res non-latins (chinois, emojis) prennent souvent plus de tokens.

### 1.2.2 ğŸ¯ Les Embeddings : L'Espace du Sens

Une fois tokenisÃ©, chaque mot est transformÃ© en un **vecteur** (une liste de nombres, par ex. 1536 dimensions pour GPT-3). C'est son **embedding**.

La magie, c'est que la position dans cet espace vectoriel capture le **sens**.

![Analogie Embeddings](images/embeddings_analogy.svg)

Le modÃ¨le "apprend" que :
- `Roi - Homme + Femme â‰ˆ Reine`
- `Paris - France + Japon â‰ˆ Tokyo`
- `function - javascript + python â‰ˆ def`

C'est cette propriÃ©tÃ© qui permet la recherche sÃ©mantique (RAG) : on peut trouver du code pertinent mÃªme si les mots-clÃ©s ne sont pas exactement les mÃªmes.

### 1.2.3 ğŸ‘ï¸ Le MÃ©canisme d'Attention

C'est le cÅ“ur du rÃ©acteur. Pour chaque token, le modÃ¨le se pose trois questions, formalisÃ©es par trois vecteurs : **Query (Q)**, **Key (K)**, et **Value (V)**.

![MÃ©canisme d'Attention](images/attention_mechanism.svg)

Imaginez une base de donnÃ©es floue :
1.  **Query (Q)** : "Je suis le token `items`. Je cherche mon contexte."
2.  **Key (K)** : Les autres tokens rÃ©pondent. "Moi `total`, je suis pertinent Ã  10%". "Moi `price`, je suis pertinent Ã  90%".
3.  **Value (V)** : On rÃ©cupÃ¨re l'information des tokens pertinents pour enrichir le sens de `items`.

GrÃ¢ce Ã  cela, quand le modÃ¨le lit `it` dans "The cat ate the mouse because it was hungry", il sait que `it` fait rÃ©fÃ©rence Ã  `cat`. Si la phrase Ã©tait "because it was delicious", `it` ferait rÃ©fÃ©rence Ã  `mouse`.

### 1.2.4 ğŸ“š La HiÃ©rarchie des Couches

Un LLM empile des dizaines de ces couches d'attention (96 pour GPT-3).

![HiÃ©rarchie des Couches](images/layer_hierarchy.svg)

*   **Couches basses** : Comprennent la syntaxe (oÃ¹ mettre les virgules, les parenthÃ¨ses).
*   **Couches moyennes** : Comprennent la sÃ©mantique (le code compile, les types correspondent).
*   **Couches hautes** : Comprennent l'intention et le raisonnement (ce que le code *fait*).

---

## âš™ï¸ 1.3 Comment un LLM GÃ©nÃ¨re du Texte

### 1.3.1 ğŸ² La PrÃ©diction du Token Suivant

Les LLMs sont des **prÃ©dicteurs statistiques**. Ils ne "rÃ©flÃ©chissent" pas avant de parler ; ils calculent la probabilitÃ© du prochain mot.

![PrÃ©diction du Token Suivant](images/next_token_prediction.svg)

Face Ã  `def calculate_`, le modÃ¨le attribue des probabilitÃ©s :
- `total` : 40%
- `sum` : 20%
- `price` : 10%
- `banana` : 0.001%

### 1.3.2 ğŸŒ¡ï¸ La TempÃ©rature : CrÃ©ativitÃ© vs PrÃ©cision

Vous pouvez contrÃ´ler comment le modÃ¨le choisit dans cette liste via la **tempÃ©rature**.

*   **TempÃ©rature 0** : Prend toujours le mot le plus probable (dÃ©terministe). IdÃ©al pour le code.
*   **TempÃ©rature 1** : Pioche parfois des mots moins probables. IdÃ©al pour la poÃ©sie ou le brainstorming.

### 1.3.3 ğŸ“ La FenÃªtre de Contexte

Le modÃ¨le a une mÃ©moire de travail limitÃ©e : la fenÃªtre de contexte.

![FenÃªtre de Contexte](images/context_window_vis.svg)

Tout ce qui sort de cette fenÃªtre (vers la gauche) est dÃ©finitivement oubliÃ©. C'est pourquoi les agents doivent utiliser des mÃ©moires externes (bases de donnÃ©es, fichiers) pour gÃ©rer des projets entiers.

---

## âš ï¸ 1.4 Les Limites et les Agents

MÃªme avec toute cette puissance, un LLM "nu" a des limites critiques :

![Limites du LLM](images/llm_limits.svg)

1.  **Connaissances figÃ©es** : Il ne connaÃ®t pas les librairies sorties aprÃ¨s sa date d'entraÃ®nement.
2.  **Pas d'action** : Il ne peut pas lancer le code qu'il Ã©crit.
3.  **Hallucinations** : Il peut inventer des faits avec une assurance totale.

### 1.4.1 La Solution : L'Agent (Boucle ReAct)

C'est lÃ  qu'intervient l'Agent. Au lieu de juste rÃ©pondre, l'agent entre dans une boucle de rÃ©flexion et d'action.

![Boucle ReAct](images/boucle_react.svg)

1.  **Think** : "Je dois vÃ©rifier si ce fichier existe."
2.  **Act** : ExÃ©cute la commande `ls`.
3.  **Observe** : Lit le rÃ©sultat rÃ©el.
4.  **Think** : "Ah, il n'existe pas. Je dois le crÃ©er."

Cette boucle permet Ã  l'agent de **vÃ©rifier ses hypothÃ¨ses** et de corriger ses erreurs, le rendant bien plus fiable qu'un simple chat.

---

## ğŸ“ 1.5 Points ClÃ©s Ã  Retenir

*   **Transformers** : Architecture parallÃ¨le basÃ©e sur l'Attention.
*   **Tokens** : UnitÃ©s de base, le coÃ»t dÃ©pend du dÃ©coupage.
*   **Probabiliste** : Le modÃ¨le prÃ©dit la suite logique, pas forcÃ©ment la vÃ©ritÃ©.
*   **Agent** : Enveloppe le LLM avec des outils et une boucle d'exÃ©cution pour compenser ses limites.

---

| â¬…ï¸ PrÃ©cÃ©dent | ğŸ“– Sommaire | â¡ï¸ Suivant |
|:-------------|:-----------:|:-----------|
| [Avant-propos](00-avant-propos.md) | [Index](README.md) | [Le RÃ´le des Agents](02-role-des-agents.md) |
