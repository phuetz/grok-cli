# Annexe A : Comprendre les Transformers

> **Note** : Cette annexe est pour ceux qui veulent comprendre le fonctionnement interne des LLMs. Elle n'est **pas n√©cessaire** pour construire des agents ‚Äî vous pouvez la sauter sans probl√®me.

---

## A.1 Une Br√®ve Histoire des Mod√®les de Langage

### A.1.1 L'√àre Statistique : Les Mod√®les N-grammes

Pendant des d√©cennies, le traitement automatique du langage naturel (NLP) reposait sur des approches purement statistiques. L'id√©e fondamentale √©tait simple : si nous pouvons compter combien de fois certaines s√©quences de mots apparaissent ensemble dans un grand corpus de texte, nous pouvons pr√©dire quel mot viendra probablement apr√®s une s√©quence donn√©e.

Les **mod√®les n-grammes** incarnaient cette philosophie. Un mod√®le bigramme (n=2) pr√©disait le mot suivant uniquement en fonction du mot pr√©c√©dent. Un mod√®le trigramme (n=3) utilisait les deux mots pr√©c√©dents. Et ainsi de suite.

Prenons un exemple concret. Supposons que nous ayons entra√Æn√© un mod√®le 5-grammes sur un corpus de textes fran√ßais. Face √† la s√©quence "le chat dort sur le", le mod√®le consulterait ses tables de fr√©quences :

- "le chat dort sur le **canap√©**" : vu 1,247 fois dans le corpus
- "le chat dort sur le **tapis**" : vu 892 fois
- "le chat dort sur le **lit**" : vu 756 fois
- "le chat dort sur le **toit**" : vu 23 fois

Le mod√®le pr√©dirait donc "canap√©" avec une probabilit√© proportionnelle √† ces fr√©quences. Simple, efficace... et profond√©ment limit√©.

Le probl√®me fondamental des n-grammes tient en un mot : **contexte**. Ces mod√®les ne peuvent "voir" qu'une fen√™tre fixe de mots ‚Äî typiquement 3 √† 5. Or, le langage humain regorge de d√©pendances √† longue distance. Consid√©rez cette phrase :

> "Le d√©veloppeur qui avait pass√© trois ans √† travailler sur ce projet, malgr√© les difficult√©s rencontr√©es avec l'√©quipe de management et les contraintes budg√©taires impos√©es par la direction, **√©tait** finalement satisfait du r√©sultat."

Le verbe "√©tait" doit s'accorder avec "Le d√©veloppeur" ‚Äî un mot situ√© √† plus de trente tokens de distance ! Aucun mod√®le n-gramme pratique ne pouvait capturer cette relation.

| Aspect | Mod√®les N-grammes | Limitation |
|--------|-------------------|------------|
| **M√©moire** | Fen√™tre fixe (3-5 mots) | Perte du contexte lointain |
| **Taille** | Croissance exponentielle | V^n entr√©es pour vocabulaire V |
| **G√©n√©ralisation** | Aucune | Ne reconna√Æt que ce qu'il a vu exactement |
| **Donn√©es rares** | Probl√©matique | "smoothing" n√©cessaire mais imparfait |

### A.1.2 Les R√©seaux R√©currents : Une Promesse Partiellement Tenue

Dans les ann√©es 2010, une nouvelle approche √©mergea : les r√©seaux de neurones r√©currents (RNN). L'id√©e √©tait √©l√©gante et biologiquement inspir√©e. Au lieu de regarder une fen√™tre fixe de mots, le r√©seau maintiendrait un **√©tat cach√©** ‚Äî une sorte de "m√©moire de travail" ‚Äî qui se propagerait d'un mot au suivant.

Les variantes comme LSTM (Long Short-Term Memory) et GRU (Gated Recurrent Unit) ajout√®rent des m√©canismes de "portes" pour mieux contr√¥ler le flux d'information.

Cependant, deux probl√®mes fondamentaux persistaient :

**Le gradient √©vanescent** : Lors de l'entra√Ænement, les signaux d'erreur doivent se propager √† travers la cha√Æne de r√©currence. √Ä chaque √©tape, ils sont multipli√©s par des poids, et si ces poids sont inf√©rieurs √† 1, le signal diminue exponentiellement. Apr√®s 50 ou 100 √©tapes, il devient pratiquement imperceptible.

**La s√©quentialit√© impos√©e** : Par construction, un RNN doit traiter les mots un par un, dans l'ordre. Cette d√©pendance s√©quentielle emp√™che toute parall√©lisation efficace.

| Crit√®re | N-grammes | RNN/LSTM | Impact pratique |
|---------|-----------|----------|-----------------|
| **Contexte** | ~5 mots | ~100-500 mots (th√©orique) | LSTM meilleur mais imparfait |
| **Parall√©lisation** | Excellente | Impossible | Entra√Ænement 10-100x plus lent |
| **M√©moire GPU** | Faible | Mod√©r√©e | LSTM plus gourmand |
| **D√©pendances longues** | Aucune | Difficiles | Gradient vanishing persiste |

### A.1.3 Juin 2017 : "Attention Is All You Need"

Le 12 juin 2017, une √©quipe de huit chercheurs chez Google publia un article au titre provocateur : **"Attention Is All You Need"**. L'article proposait une architecture radicalement diff√©rente appel√©e **Transformer**.

L'id√©e centrale : et si on abandonnait compl√®tement la r√©currence ? Et si, au lieu de traiter les mots s√©quentiellement, on les traitait **tous en parall√®le**, en utilisant uniquement des m√©canismes d'attention pour capturer les relations entre eux ?

Les r√©sultats furent spectaculaires :

| M√©trique | LSTM (meilleur) | Transformer | Am√©lioration |
|----------|-----------------|-------------|--------------|
| BLEU (EN‚ÜíDE) | 25.8 | 28.4 | +10% |
| BLEU (EN‚ÜíFR) | 41.0 | 41.8 | +2% |
| Temps d'entra√Ænement | ~3 semaines | 3.5 jours | **~6x plus rapide** |
| Param√®tres | ~200M | 65M | 3x moins |

---

## A.2 L'Anatomie d'un Transformer

### A.2.1 La Tokenisation : D√©couper le Langage

Avant m√™me d'entrer dans le r√©seau de neurones, le texte doit √™tre converti en nombres. Cette √©tape, appel√©e **tokenisation**, est plus subtile qu'il n'y para√Æt.

**Le probl√®me du vocabulaire**

Une approche na√Øve consisterait √† attribuer un identifiant unique √† chaque mot du dictionnaire. Mais cette strat√©gie se heurte √† plusieurs obstacles :

1. **La taille du vocabulaire** : Le fran√ßais compte environ 100,000 mots courants.
2. **Les mots rares** : De nombreux mots ne seront vus qu'une ou deux fois pendant l'entra√Ænement.
3. **Les langues agglutinantes** : En allemand, finnois ou turc, les mots peuvent √™tre compos√©s de nombreux morph√®mes.

**La solution : Byte-Pair Encoding (BPE)**

L'algorithme fonctionne ainsi :
1. Commencer avec un vocabulaire contenant uniquement les caract√®res individuels
2. Compter toutes les paires de tokens adjacents dans le corpus
3. Fusionner la paire la plus fr√©quente en un nouveau token
4. R√©p√©ter jusqu'√† atteindre la taille de vocabulaire d√©sir√©e

**Implications pratiques pour les d√©veloppeurs :**

| Impact | Description | Conseil pratique |
|--------|-------------|------------------|
| **Co√ªt** | Les API facturent par token | Noms de variables courts = moins cher |
| **Limite de contexte** | 128K tokens ‚â† 128K caract√®res | Un fichier de 10KB peut consommer 3-5K tokens |
| **Langues** | Non-anglais = plus de tokens | Budget 30-50% de tokens en plus pour le fran√ßais |
| **Code** | Syntaxe verbale = plus de tokens | `calculateTotalAmountWithTax` = ~8 tokens |
| **Comptage** | LLMs comptent mal les caract√®res | "Combien de 'r' dans strawberry ?" ‚Üí souvent faux |

### A.2.2 Les Embeddings : Transformer les Symboles en Vecteurs de Sens

Une fois le texte tokenis√©, chaque identifiant num√©rique doit √™tre converti en une repr√©sentation que le r√©seau de neurones peut manipuler. Cette repr√©sentation prend la forme d'un **embedding** : un vecteur dense de nombres r√©els.

**La magie √©mergente des embeddings**

Les mots ayant des significations similaires se retrouvent proches dans l'espace vectoriel. L'exemple classique est l'analogie "roi - homme + femme ‚âà reine".

Pour le code, cette propri√©t√© est pr√©cieuse :

| Relation | Exemples |
|----------|----------|
| √âquivalence cross-langage | `array.push` (JS) ‚âà `list.append` (Python) |
| Patterns de conception | `async/await` ‚âà `Promise` ‚âà `.then().catch()` |
| Op√©rations similaires | `console.log` ‚âà `print` ‚âà `System.out.println` |

---

## A.3 Le M√©canisme d'Attention

### A.3.1 L'Intuition : Une Base de Donn√©es Associative

Pour comprendre l'attention, une analogie avec les bases de donn√©es est utile :

- **Query (Q)** : "Que cherche-t-on ?" ‚Äî Ce que le token actuel veut savoir
- **Key (K)** : "Qu'avons-nous ?" ‚Äî Ce que chaque token peut offrir comme contexte
- **Value (V)** : "Quel contenu ?" ‚Äî L'information effectivement transmise

### A.3.2 La M√©canique Math√©matique

Pour chaque token, trois vecteurs sont calcul√©s :

```
Q = X √ó W_Q    (query)
K = X √ó W_K    (key)
V = X √ó W_V    (value)
```

L'attention est ensuite calcul√©e par :

```
Attention(Q, K, V) = softmax(Q √ó K^T / ‚àöd_k) √ó V
```

### A.3.3 Multi-Head Attention

Une seule "t√™te" d'attention capture une seule fa√ßon de relier les tokens. La solution est d'utiliser plusieurs t√™tes en parall√®le, chacune avec ses propres matrices de projection.

| T√™te | Sp√©cialisation observ√©e | Exemple |
|------|-------------------------|---------|
| T√™te 1 | D√©pendances syntaxiques | sujet ‚Üí verbe |
| T√™te 2 | R√©solution de cor√©f√©rences | "il" ‚Üí "d√©veloppeur" |
| T√™te 3 | Relations s√©mantiques | "Python" ‚Üí "code" |
| T√™te 4 | Positions relatives | mot[i] ‚Üí mot[i-1] |

---

## A.4 Scaling Laws

Des chercheurs d'OpenAI et d'Anthropic ont montr√© que les performances suivent des relations math√©matiques pr√©visibles :

| Axe | Description | Effet sur la performance |
|-----|-------------|--------------------------|
| **Param√®tres (N)** | Nombre de poids du mod√®le | L ~ N^(-0.076) |
| **Donn√©es (D)** | Tokens d'entra√Ænement | L ~ D^(-0.095) |
| **Compute (C)** | FLOPs d'entra√Ænement | L ~ C^(-0.050) |

| Mod√®le | Param√®tres | Tokens d'entra√Ænement | Ratio Tokens/Params |
|--------|------------|----------------------|---------------------|
| GPT-3 | 175B | 300B | 1.7 |
| Chinchilla | 70B | 1.4T | 20 |
| LLaMA 2 | 70B | 2T | 29 |
| GPT-4 | ~1.8T (rumeur) | ~13T | ~7 |

---

## A.5 Les Hallucinations : Pourquoi les LLMs "Mentent"

Il est crucial de comprendre ce que fait r√©ellement un LLM : il pr√©dit le token le plus probable √©tant donn√© le contexte. Il n'a pas de "base de connaissances" s√©par√©e qu'il consulte, pas de m√©canisme pour v√©rifier la v√©racit√© de ses affirmations.

| Cause | Explication | Exemple |
|-------|-------------|---------|
| **Pression de compl√©tion** | Le mod√®le doit toujours produire quelque chose | Invente plut√¥t que de dire "je ne sais pas" |
| **M√©lange de patterns** | Combine des informations de sources diff√©rentes | Attribue une citation √† la mauvaise personne |
| **G√©n√©ralisation excessive** | Extrapole au-del√† des donn√©es vues | "Python 4.0 a introduit..." (n'existe pas) |
| **Manque de grounding** | Pas de connexion au monde r√©el | Ignore les √©v√©nements post-training |

---

## A.6 Panorama des Mod√®les 2025

### Mod√®les Propri√©taires (API Cloud)

| Mod√®le | √âditeur | Forces | Co√ªt (1M tokens) |
|--------|---------|--------|------------------|
| **GPT-4o** | OpenAI | Polyvalent, multimodal, rapide | ~$5-15 |
| **Claude 3.5 Sonnet** | Anthropic | Code excellent, 200K contexte | ~$3-15 |
| **Gemini 1.5 Pro** | Google | 1M tokens contexte | ~$3.5-10.5 |
| **Grok-2** | xAI | Acc√®s temps r√©el (X/Twitter) | ~$2-10 |

### Mod√®les Open Source

| Mod√®le | Param√®tres | Licence | Forces |
|--------|------------|---------|--------|
| **Llama 3.1** | 8B/70B/405B | Meta Llama 3.1 | Polyvalent |
| **Mistral Large 2** | 123B | Apache 2.0 | Multilingue |
| **DeepSeek V3** | 685B (MoE) | MIT | √âtat de l'art open |
| **Phi-3** | 3.8B-14B | MIT | Compact, performant |

---

## A.7 R√©f√©rences

| Source | Description |
|--------|-------------|
| Vaswani et al. (2017) | "Attention Is All You Need" ‚Äî L'article fondateur |
| Kaplan et al. (2020) | "Scaling Laws for Neural Language Models" |
| Hoffmann et al. (2022) | "Training Compute-Optimal LLMs" (Chinchilla) |
| Wei et al. (2022) | "Emergent Abilities of Large Language Models" |
| Ji et al. (2023) | "Survey of Hallucination in NLG" |

---

[üìö Table des Mati√®res](README.md) | [‚û°Ô∏è Annexe B : Glossaire](glossaire.md)
