# Chapitre 7 ‚Äî RAG Moderne üîç

---

## üé¨ Sc√®ne d'ouverture

*Mercredi, 10h du matin. Lina teste son agent sur le projet r√©el de son √©quipe ‚Äî un syst√®me de paiement en production.*

**Lina** : "Explique-moi comment fonctionne la fonction `processPayment`."

*L'agent r√©pond imm√©diatement, avec assurance :*

```
La fonction processPayment utilise l'API Stripe pour traiter les paiements.
Elle prend un objet PaymentIntent, v√©rifie le montant, et appelle
stripe.paymentIntents.create() avec les param√®tres appropri√©s...
```

*Lina se fige.*

**Lina** *(relisant lentement)* : "Stripe... PaymentIntent... stripe.paymentIntents.create..."

*Elle ouvre le vrai fichier payment-processor.ts dans son projet.*

**Lina** : "On n'utilise PAS Stripe. On utilise Adyen. Et la fonction s'appelle `submitTransaction`, pas `processPayment`."

*Elle se tourne vers Marc qui passe avec son caf√©.*

**Lina** : "Il a tout invent√©. Pas un seul mot de sa r√©ponse n'est vrai."

**Marc** *(s'arr√™tant)* : "Qu'est-ce que tu lui as demand√© ?"

**Lina** : "D'expliquer notre fonction de paiement. Et il m'a d√©crit une int√©gration Stripe compl√®te ‚Äî avec des d√©tails tr√®s convaincants. Sauf que c'est de la fiction."

**Marc** *(posant son caf√©)* : "C'est normal. Le LLM ne conna√Æt pas ton code."

**Lina** : "Mais il a acc√®s au projet. Je suis dans le r√©pertoire du projet."

**Marc** : "Non. Il a acc√®s √† son **entra√Ænement** ‚Äî des millions de repos GitHub, de la documentation, des tutoriels. Quand tu dis 'payment', il te donne ce qu'il a vu le plus souvent. Et c'est probablement Stripe."

*Lina r√©alise l'ampleur du probl√®me.*

**Lina** : "Donc chaque fois qu'il parle de mon code... il invente ?"

**Marc** : "Il **extrapole** √† partir de ce qu'il conna√Æt. C'est ce qu'on appelle l'hallucination. Pas m√©chant ‚Äî juste... ignorant de ton contexte."

**Lina** : "Alors comment les outils comme Cursor ou Copilot font ? Ils connaissent vraiment le code."

**Marc** *(s'asseyant)* : "Ils ne se contentent pas du LLM. Avant de poser la question au mod√®le, ils **cherchent** dans ton code les morceaux pertinents. Puis ils injectent ces morceaux dans le prompt."

**Lina** : "Donc le mod√®le voit mon vrai code ?"

**Marc** : "Exactement. C'est ce qu'on appelle **RAG** ‚Äî Retrieval-Augmented Generation. Tu r√©cup√®res d'abord, tu g√©n√®res ensuite."

*Lina ouvre son carnet.*

**Lina** : "Montre-moi comment √ßa marche."

**Marc** : "C'est un rabbit hole. Embeddings, similarit√© cosinus, chunking, re-ranking... Tu veux vraiment plonger ?"

**Lina** *(souriant)* : "On a bien plong√© dans MCTS. √áa ne peut pas √™tre pire."

**Marc** : "Oh, tu serais surprise."

---

## üìã Table des mati√®res

| Section | Titre | Description |
|:-------:|-------|-------------|
| 7.1 | üö´ Le Probl√®me du Contexte | Pourquoi le LLM seul ne suffit pas |
| 7.2 | üßÆ Embeddings | La fondation math√©matique du RAG |
| 7.3 | üîÑ Pipeline RAG | Les phases d'indexation et retrieval |
| 7.4 | üîÄ Retrieval Hybride | Combiner s√©mantique et keywords |
| 7.5 | üíâ Augmentation | Injecter le contexte dans le prompt |
| 7.6 | üõ†Ô∏è Impl√©mentation | Le module RAG de Grok-CLI |
| 7.7 | üìä √âvaluation | Mesurer la qualit√© du retrieval |

---

## 7.1 üö´ Le Probl√®me du Contexte

### 7.1.1 Les limites du LLM seul

Un LLM, aussi puissant soit-il, souffre de plusieurs limitations fondamentales lorsqu'il s'agit de travailler sur votre code. Ces limitations ne sont pas des bugs √† corriger, mais des caract√©ristiques intrins√®ques de la fa√ßon dont ces mod√®les fonctionnent.

**Premi√®rement, la connaissance est fig√©e.** Le mod√®le a √©t√© entra√Æn√© sur des donn√©es jusqu'√† une certaine date (le "cutoff"). Il ne conna√Æt pas les nouvelles versions de frameworks, les CVE r√©centes, ou les changements d'API. Demandez-lui la derni√®re version de React, et il vous donnera peut-√™tre une version datant d'un an.

**Deuxi√®mement, il n'a pas acc√®s √† votre code priv√©.** Votre `AuthService`, votre `PaymentProcessor`, vos conventions d'√©quipe ‚Äî tout cela est invisible pour lui. Quand vous posez une question sur votre code, il ne peut qu'**halluciner** une r√©ponse plausible bas√©e sur ce qu'il a vu dans des projets similaires.

![Limites du LLM seul - g√©n√©r√© par Nanobanana](images/llm_limits.svg)

### 7.1.2 La solution RAG

**RAG** (Retrieval-Augmented Generation) r√©sout ces probl√®mes en ajoutant une √©tape de r√©cup√©ration avant la g√©n√©ration. L'id√©e est simple mais puissante : plut√¥t que de compter sur la m√©moire du mod√®le, on va **chercher** les informations pertinentes et les **injecter** dans le contexte.

C'est comme la diff√©rence entre passer un examen √† livre ferm√© (le LLM seul) et √† livre ouvert (RAG). Dans le second cas, vous avez acc√®s √† vos notes ‚Äî √† condition de savoir o√π chercher.

![Architecture RAG g√©n√©r√©e par Nanobanana](images/rag_pipeline_detail.svg)

| √âtape | Action | R√©sultat |
|:-----:|--------|----------|
| 1Ô∏è‚É£ **Retrieve** | Chercher dans la base de code | Documents pertinents |
| 2Ô∏è‚É£ **Augment** | Injecter dans le prompt | Contexte enrichi |
| 3Ô∏è‚É£ **Generate** | G√©n√©rer la r√©ponse | R√©ponse pr√©cise |

---

## 7.2 üßÆ Embeddings : La Fondation du RAG

### 7.2.1 Qu'est-ce qu'un embedding ?

Pour rechercher du code s√©mantiquement (par le sens, pas juste par mots-cl√©s), nous avons besoin de repr√©senter le texte sous forme de nombres. C'est le r√¥le des **embeddings**.

Un embedding est un **vecteur de nombres** (typiquement 384 √† 3072 dimensions) qui capture le "sens" d'un texte. Deux textes similaires auront des vecteurs proches dans cet espace √† haute dimension.

![Embeddings Visualization - g√©n√©r√© par Nanobanana](images/embeddings_viz.svg)

### 7.2.2 Similarit√© cosine

Pour comparer deux embeddings, on utilise la **similarit√© cosine**. Elle mesure l'angle entre deux vecteurs, ind√©pendamment de leur magnitude.

![Similarit√© cosine](images/cosine-similarity.svg)

**Impl√©mentation TypeScript :**

```typescript
// src/embeddings/similarity.ts

function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

// Utilisation
const embeddingA = await embed("function calculateTotal()");
const embeddingB = await embed("function computeSum()");

const similarity = cosineSimilarity(embeddingA, embeddingB);
console.log(`Similarit√©: ${similarity}`);  // ~0.85
```

### 7.2.3 Mod√®les d'embedding

Le choix du mod√®le d'embedding impacte directement la qualit√© du retrieval. Voici les principaux :

| Mod√®le | Dimensions | Sp√©cialisation | Co√ªt | Performance |
|--------|:----------:|----------------|------|-------------|
| üÜì all-MiniLM-L6-v2 | 384 | G√©n√©ral | Gratuit (local) | ‚≠ê‚≠ê‚≠ê |
| üíµ text-embedding-3-small | 1536 | G√©n√©ral | $0.02/1M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê |
| üíµ text-embedding-3-large | 3072 | Haute pr√©cision | $0.13/1M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| üÜì CodeBERT | 768 | Code | Gratuit (local) | ‚≠ê‚≠ê‚≠ê‚≠ê (code) |
| üÜì StarCoder-embed | 1024 | Code | Gratuit (local) | ‚≠ê‚≠ê‚≠ê‚≠ê (code) |

> üí° **Conseil** : Pour le code, privil√©giez un mod√®le sp√©cialis√© comme CodeBERT. Il comprend mieux les noms de variables, la syntaxe et les patterns de code.

### 7.2.4 Embedding local avec Transformers.js

Pour √©viter les co√ªts API et les probl√®mes de latence, Grok-CLI utilise des embeddings locaux :

```typescript
// src/embeddings/local-embedder.ts
import { pipeline } from '@xenova/transformers';

export class LocalEmbedder {
  private model: any;
  private modelName = 'Xenova/all-MiniLM-L6-v2';
  private initialized = false;

  /**
   * Initialise le mod√®le d'embedding.
   * Cette op√©ration t√©l√©charge le mod√®le si n√©cessaire (~90MB).
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;

    console.log('üîÑ Chargement du mod√®le d\'embedding...');
    this.model = await pipeline('feature-extraction', this.modelName);
    this.initialized = true;
    console.log('‚úÖ Mod√®le charg√©');
  }

  /**
   * G√©n√®re l'embedding pour un texte.
   * @param text - Le texte √† encoder
   * @returns Vecteur de 384 dimensions
   */
  async embed(text: string): Promise<number[]> {
    if (!this.initialized) {
      await this.initialize();
    }

    const output = await this.model(text, {
      pooling: 'mean',     // Moyenne des tokens
      normalize: true       // Normaliser pour cosine
    });

    return Array.from(output.data);
  }

  /**
   * G√©n√®re les embeddings pour plusieurs textes.
   * Plus efficace que d'appeler embed() en boucle.
   */
  async embedBatch(texts: string[]): Promise<number[][]> {
    const results: number[][] = [];

    // Traitement par batch de 32 pour optimiser la m√©moire
    const batchSize = 32;
    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);
      const batchResults = await Promise.all(
        batch.map(text => this.embed(text))
      );
      results.push(...batchResults);
    }

    return results;
  }
}
```

---

## 7.3 üîÑ Pipeline RAG pour le Code

### 7.3.1 Vue d'ensemble

Le pipeline RAG pour le code se d√©compose en deux phases principales : l'**indexation** (offline, une seule fois) et le **retrieval** (online, √† chaque requ√™te).

![Pipeline RAG Code](images/rag-pipeline-code.svg)

| Phase | √âtapes | Fr√©quence |
|-------|--------|-----------|
| üì¶ **Indexation** | Parse ‚Üí Chunk ‚Üí Embed ‚Üí Store | Une fois + incr√©mental |
| üîé **Retrieval** | Embed ‚Üí Search ‚Üí Rerank ‚Üí Return | Chaque requ√™te |

### 7.3.2 Chunking du code : l'art du d√©coupage

Le **chunking** (d√©coupage) est crucial. Un mauvais chunking produit de mauvais r√©sultats, m√™me avec le meilleur mod√®le d'embedding.

![Comparaison des strat√©gies de chunking](images/chunking-comparison.svg)

**Impl√©mentation du chunker AST :**

```typescript
// src/context/chunker.ts
import * as parser from '@typescript-eslint/parser';

interface Chunk {
  id: string;
  type: 'function' | 'class' | 'method' | 'variable' | 'type';
  name: string;
  content: string;
  filePath: string;
  startLine: number;
  endLine: number;
  docstring?: string;
}

export class ASTChunker {
  /**
   * D√©coupe un fichier de code en chunks logiques via l'AST.
   * Chaque fonction, classe, m√©thode devient un chunk s√©par√©.
   */
  chunk(code: string, filePath: string): Chunk[] {
    const ast = parser.parse(code, {
      sourceType: 'module',
      ecmaVersion: 'latest',
      range: true,
      loc: true
    });

    const chunks: Chunk[] = [];

    // Traverser l'AST √† la recherche de n≈ìuds "chunkables"
    for (const node of ast.body) {
      if (this.isChunkableNode(node)) {
        chunks.push(this.createChunk(node, code, filePath));
      }

      // G√©rer les classes avec m√©thodes
      if (node.type === 'ClassDeclaration' && node.body) {
        for (const member of node.body.body) {
          if (member.type === 'MethodDefinition') {
            chunks.push(this.createChunk(member, code, filePath));
          }
        }
      }
    }

    return chunks;
  }

  private isChunkableNode(node: any): boolean {
    const chunkableTypes = [
      'FunctionDeclaration',
      'ClassDeclaration',
      'MethodDefinition',
      'ExportNamedDeclaration',
      'ExportDefaultDeclaration',
      'TSInterfaceDeclaration',
      'TSTypeAliasDeclaration'
    ];
    return chunkableTypes.includes(node.type);
  }

  private createChunk(node: any, code: string, filePath: string): Chunk {
    const content = code.slice(node.range[0], node.range[1]);

    return {
      id: `${filePath}:${node.loc.start.line}`,
      type: this.getNodeType(node),
      name: this.getNodeName(node),
      content,
      filePath,
      startLine: node.loc.start.line,
      endLine: node.loc.end.line,
      docstring: this.extractDocstring(code, node.range[0])
    };
  }

  private getNodeType(node: any): Chunk['type'] {
    switch (node.type) {
      case 'FunctionDeclaration': return 'function';
      case 'ClassDeclaration': return 'class';
      case 'MethodDefinition': return 'method';
      case 'TSInterfaceDeclaration':
      case 'TSTypeAliasDeclaration': return 'type';
      default: return 'variable';
    }
  }

  private getNodeName(node: any): string {
    if (node.id?.name) return node.id.name;
    if (node.key?.name) return node.key.name;
    if (node.declaration?.id?.name) return node.declaration.id.name;
    return 'anonymous';
  }

  private extractDocstring(code: string, nodeStart: number): string | undefined {
    // Chercher un commentaire JSDoc avant le n≈ìud
    const beforeNode = code.slice(Math.max(0, nodeStart - 500), nodeStart);
    const jsdocMatch = beforeNode.match(/\/\*\*[\s\S]*?\*\/\s*$/);
    return jsdocMatch?.[0];
  }
}
```

### 7.3.3 M√©tadonn√©es enrichies

Chaque chunk stocke des m√©tadonn√©es qui am√©liorent le retrieval et permettent l'expansion contextuelle :

```typescript
// src/context/types.ts

interface CodeChunk {
  // üè∑Ô∏è Identit√©
  id: string;              // Identifiant unique
  filePath: string;        // Chemin du fichier source
  name: string;            // Nom de la fonction/classe
  type: ChunkType;         // function | class | method | type

  // üìù Contenu
  content: string;         // Code source complet
  docstring?: string;      // Documentation JSDoc
  signature?: string;      // Signature (pour fonctions)

  // üìç Position
  startLine: number;       // Ligne de d√©but
  endLine: number;         // Ligne de fin

  // üîó Relations (pour expansion)
  imports: string[];       // Modules import√©s
  exports: string[];       // Symbols export√©s
  calls: string[];         // Fonctions appel√©es
  calledBy?: string[];     // Qui appelle cette fonction

  // üßÆ Embedding
  embedding: number[];     // Vecteur 384-3072 dimensions

  // üìä M√©triques
  complexity?: number;     // Complexit√© cyclomatique
  lastModified: Date;      // Date de modification
}

type ChunkType = 'function' | 'class' | 'method' | 'variable' | 'type';
```

| Cat√©gorie | Champs | Utilit√© |
|-----------|--------|---------|
| üè∑Ô∏è **Identit√©** | id, filePath, name, type | Identifier et filtrer |
| üìù **Contenu** | content, docstring, signature | Afficher et matcher |
| üìç **Position** | startLine, endLine | Navigation dans l'IDE |
| üîó **Relations** | imports, calls, calledBy | Expansion contextuelle |
| üßÆ **Vector** | embedding | Recherche s√©mantique |
| üìä **M√©triques** | complexity, lastModified | Priorisation |

---

## 7.4 üîÄ Retrieval Hybride

### 7.4.1 Les limites du retrieval s√©mantique seul

Le retrieval par embeddings seul pr√©sente des faiblesses importantes, particuli√®rement pour le code :

![Limites du retrieval semantique pur](images/semantic-retrieval-limits.svg)

### 7.4.2 Retrieval hybride : s√©mantique + keywords

La solution : combiner retrieval s√©mantique et par mots-cl√©s avec une technique appel√©e **Reciprocal Rank Fusion (RRF)**.

![Hybrid Retrieval g√©n√©r√© par Nanobanana](images/hybrid_retrieval.svg)

**Impl√©mentation :**

```typescript
// src/context/hybrid-retriever.ts

interface RetrievedChunk extends CodeChunk {
  semanticScore?: number;
  keywordScore?: number;
  fusedScore?: number;
}

export class HybridRetriever {
  // Poids relatifs des deux m√©thodes
  private semanticWeight = 0.7;  // 70% s√©mantique
  private keywordWeight = 0.3;   // 30% keywords

  async retrieve(query: string, limit: number = 10): Promise<RetrievedChunk[]> {
    // 1. Retrieval s√©mantique (embeddings + cosine similarity)
    const semanticResults = await this.semanticSearch(query, limit * 2);

    // 2. Retrieval par keywords (BM25)
    const keywordResults = await this.keywordSearch(query, limit * 2);

    // 3. Fusion avec Reciprocal Rank Fusion
    const fused = this.fuseResults(semanticResults, keywordResults);

    // 4. Retourner les top K
    return fused.slice(0, limit);
  }

  /**
   * Reciprocal Rank Fusion (RRF)
   * Score = Œ£ 1/(k + rank)
   * k = 60 est la valeur standard qui donne de bons r√©sultats
   */
  private fuseResults(
    semantic: RetrievedChunk[],
    keyword: RetrievedChunk[]
  ): RetrievedChunk[] {
    const scores = new Map<string, number>();
    const k = 60; // Constante RRF standard

    // Ajouter les scores s√©mantiques
    semantic.forEach((chunk, rank) => {
      const current = scores.get(chunk.id) ?? 0;
      scores.set(chunk.id, current + this.semanticWeight / (k + rank));
    });

    // Ajouter les scores keywords
    keyword.forEach((chunk, rank) => {
      const current = scores.get(chunk.id) ?? 0;
      scores.set(chunk.id, current + this.keywordWeight / (k + rank));
    });

    // Construire la map de tous les chunks
    const allChunks = new Map<string, RetrievedChunk>();
    [...semantic, ...keyword].forEach(c => allChunks.set(c.id, c));

    // Trier par score fusionn√© d√©croissant
    return Array.from(scores.entries())
      .sort((a, b) => b[1] - a[1])
      .map(([id, score]) => ({
        ...allChunks.get(id)!,
        fusedScore: score
      }));
  }

  private async semanticSearch(query: string, limit: number): Promise<RetrievedChunk[]> {
    const queryEmbedding = await this.embedder.embed(query);

    return this.db.query(`
      SELECT *, cosine_similarity(embedding, ?) as semanticScore
      FROM code_chunks
      ORDER BY semanticScore DESC
      LIMIT ?
    `, [queryEmbedding, limit]);
  }

  private async keywordSearch(query: string, limit: number): Promise<RetrievedChunk[]> {
    // Tokenizer adapt√© au code (camelCase, snake_case)
    const tokens = this.tokenizeCode(query);

    // BM25 via SQLite FTS5
    return this.db.query(`
      SELECT *, bm25(code_chunks_fts) as keywordScore
      FROM code_chunks_fts
      WHERE code_chunks_fts MATCH ?
      ORDER BY keywordScore DESC
      LIMIT ?
    `, [tokens.join(' OR '), limit]);
  }

  /**
   * Tokenizer sp√©cialis√© pour le code
   * "getUserById" ‚Üí ["get", "user", "by", "id", "getuserbyid"]
   */
  private tokenizeCode(text: string): string[] {
    const tokens = new Set<string>();

    // Garder le terme original
    tokens.add(text.toLowerCase());

    // Splitter camelCase et snake_case
    const parts = text
      .replace(/([a-z])([A-Z])/g, '$1 $2')  // camelCase
      .replace(/_/g, ' ')                     // snake_case
      .toLowerCase()
      .split(/\s+/)
      .filter(t => t.length > 2);

    parts.forEach(p => tokens.add(p));

    return Array.from(tokens);
  }
}
```

### 7.4.3 Reranking avec Cross-Encoder

Pour affiner davantage les r√©sultats, on peut utiliser un **cross-encoder**. Contrairement aux embeddings (bi-encoder) qui encodent query et document s√©par√©ment, le cross-encoder les compare directement ensemble.

![Reranking avec Cross-Encoder](images/cross-encoder-reranking.svg)

```typescript
// src/context/reranker.ts

export class CrossEncoderReranker {
  private model: any;  // cross-encoder model

  /**
   * Rerank les candidats avec un cross-encoder.
   * Plus lent mais plus pr√©cis que le bi-encoder.
   */
  async rerank(
    query: string,
    candidates: RetrievedChunk[],
    topK: number
  ): Promise<RetrievedChunk[]> {
    // Score chaque paire (query, document) directement
    const scored = await Promise.all(
      candidates.map(async chunk => {
        const score = await this.model.predict({
          text: query,
          text_pair: chunk.content
        });
        return { chunk, score };
      })
    );

    // Trier par score d√©croissant et retourner top K
    return scored
      .sort((a, b) => b.score - a.score)
      .slice(0, topK)
      .map(s => ({ ...s.chunk, rerankScore: s.score }));
  }
}
```

| M√©thode | Vitesse | Pr√©cision | Usage |
|---------|:-------:|:---------:|-------|
| Bi-Encoder | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Recherche initiale (top 50) |
| Cross-Encoder | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Reranking final (top 5) |

---

## 7.5 üíâ Augmentation du Prompt

### 7.5.1 Injection de contexte

Une fois les documents r√©cup√©r√©s, il faut les **injecter intelligemment** dans le prompt. L'ordre, le formatage et les instructions impactent directement la qualit√© de la r√©ponse.

```typescript
// src/context/augmenter.ts

function buildAugmentedPrompt(
  query: string,
  retrievedChunks: RetrievedChunk[]
): string {
  // Formater chaque chunk avec ses m√©tadonn√©es
  const contextSection = retrievedChunks.map((chunk, index) => `
### üìÑ ${index + 1}. ${chunk.filePath}
**Type**: ${chunk.type} | **Nom**: \`${chunk.name}\` | **Lignes**: ${chunk.startLine}-${chunk.endLine}

\`\`\`${getLanguageFromPath(chunk.filePath)}
${chunk.content}
\`\`\`
`).join('\n---\n');

  return `
Tu es un assistant de d√©veloppement expert. Utilise UNIQUEMENT le contexte fourni pour r√©pondre.

## üìö Contexte du Codebase

${contextSection}

## ‚ùì Question

${query}

## üìã Instructions
- Base ta r√©ponse UNIQUEMENT sur le contexte fourni ci-dessus
- Si l'information n'est pas dans le contexte, dis-le clairement
- Cite les fichiers et num√©ros de ligne quand tu fais r√©f√©rence au code
- Si plusieurs fichiers sont pertinents, explique leurs relations
`;
}

function getLanguageFromPath(path: string): string {
  const ext = path.split('.').pop();
  const langMap: Record<string, string> = {
    ts: 'typescript',
    tsx: 'typescript',
    js: 'javascript',
    jsx: 'javascript',
    py: 'python',
    go: 'go',
    rs: 'rust'
  };
  return langMap[ext ?? ''] ?? '';
}
```

### 7.5.2 Gestion de la limite de tokens

Les mod√®les ont une limite de contexte (128K pour GPT-4, 200K pour Claude). Il faut s√©lectionner intelligemment les chunks pour ne pas la d√©passer :

```typescript
// src/context/token-manager.ts

function fitToTokenLimit(
  chunks: RetrievedChunk[],
  query: string,
  maxTokens: number
): RetrievedChunk[] {
  const encoder = getTokenEncoder();

  // R√©server des tokens pour la query et le template
  const queryTokens = encoder.encode(query).length;
  const templateOverhead = 500;  // ~500 tokens pour les instructions
  const availableTokens = maxTokens - queryTokens - templateOverhead;

  const selected: RetrievedChunk[] = [];
  let totalTokens = 0;

  // Ajouter les chunks par ordre de pertinence
  for (const chunk of chunks) {
    const chunkTokens = encoder.encode(chunk.content).length;

    if (totalTokens + chunkTokens <= availableTokens) {
      selected.push(chunk);
      totalTokens += chunkTokens;
    } else if (totalTokens < availableTokens * 0.9) {
      // Si on a de la place, tronquer le dernier chunk
      const remaining = availableTokens - totalTokens;
      if (remaining > 100) {
        const truncated = truncateToTokens(chunk.content, remaining);
        selected.push({
          ...chunk,
          content: truncated + '\n// ... (tronqu√©)',
          truncated: true
        });
      }
      break;
    }
  }

  return selected;
}
```

![Budget tokens](images/token-budget.svg)

---

## 7.6 üõ†Ô∏è Impl√©mentation Grok-CLI

### 7.6.1 Architecture du module RAG

![Architecture du module RAG](images/rag-module-architecture.svg)

### 7.6.2 Indexeur de codebase

L'indexeur parcourt le projet, d√©coupe le code et stocke les embeddings :

```typescript
// src/context/codebase-rag/indexer.ts

interface IndexingResult {
  files: number;
  chunks: number;
  errors: number;
  duration: number;
}

export class CodebaseIndexer {
  private chunker: ASTChunker;
  private embedder: Embedder;
  private store: VectorStore;

  /**
   * Indexe un r√©pertoire complet.
   * Parcourt tous les fichiers de code et g√©n√®re leurs embeddings.
   */
  async indexDirectory(dirPath: string): Promise<IndexingResult> {
    const startTime = Date.now();
    const stats = { files: 0, chunks: 0, errors: 0, duration: 0 };

    // Trouver tous les fichiers de code
    const files = await glob('**/*.{ts,js,tsx,jsx,py,go,rs,java}', {
      cwd: dirPath,
      ignore: [
        'node_modules/**',
        'dist/**',
        'build/**',
        '.git/**',
        '*.test.*',
        '*.spec.*'
      ]
    });

    console.log(`üìÅ ${files.length} fichiers √† indexer...`);

    // Traiter par batch pour optimiser la m√©moire
    const batchSize = 10;
    for (let i = 0; i < files.length; i += batchSize) {
      const batch = files.slice(i, i + batchSize);

      await Promise.all(batch.map(async file => {
        try {
          await this.indexFile(path.join(dirPath, file));
          stats.files++;
        } catch (error) {
          console.error(`‚ùå Erreur ${file}:`, error);
          stats.errors++;
        }
      }));

      // Progress
      const progress = Math.round((i + batch.length) / files.length * 100);
      console.log(`‚è≥ ${progress}% (${stats.chunks} chunks)...`);
    }

    stats.duration = Date.now() - startTime;
    console.log(`‚úÖ Indexation termin√©e en ${stats.duration}ms`);

    return stats;
  }

  private async indexFile(filePath: string): Promise<void> {
    const content = await fs.readFile(filePath, 'utf-8');

    // 1. Chunker le fichier via AST
    const chunks = this.chunker.chunk(content, filePath);

    // 2. G√©n√©rer les embeddings
    const textsForEmbedding = chunks.map(c => this.formatForEmbedding(c));
    const embeddings = await this.embedder.embedBatch(textsForEmbedding);

    // 3. Stocker dans la base
    for (let i = 0; i < chunks.length; i++) {
      await this.store.upsert({
        ...chunks[i],
        embedding: embeddings[i]
      });
    }
  }

  /**
   * Formate un chunk pour l'embedding.
   * Inclut le type et le nom pour un meilleur matching s√©mantique.
   */
  private formatForEmbedding(chunk: Chunk): string {
    const parts = [
      `${chunk.type} ${chunk.name}`,           // "function calculateTotal"
      chunk.docstring ?? '',                    // JSDoc si pr√©sent
      chunk.content.slice(0, 500)               // Premiers 500 chars du code
    ];
    return parts.filter(Boolean).join('\n');
  }

  /**
   * Met √† jour un seul fichier (pour les changements incr√©mentaux).
   */
  async updateFile(filePath: string): Promise<void> {
    // Supprimer les anciens chunks de ce fichier
    await this.store.deleteByFile(filePath);

    // R√©indexer
    await this.indexFile(filePath);
  }
}
```

### 7.6.3 Retriever complet

Le retriever combine toutes les techniques vues pr√©c√©demment :

```typescript
// src/context/codebase-rag/retriever.ts

interface RetrievalOptions {
  topK?: number;           // Nombre de r√©sultats (d√©faut: 5)
  minScore?: number;       // Score minimum (d√©faut: 0.5)
  fileFilter?: string[];   // Filtrer par patterns de fichiers
  typeFilter?: ChunkType[]; // Filtrer par type (function, class, etc.)
  expandDependencies?: boolean; // Inclure les imports
}

export class CodebaseRetriever {
  private store: VectorStore;
  private embedder: Embedder;
  private reranker: CrossEncoderReranker;

  async retrieve(
    query: string,
    options: RetrievalOptions = {}
  ): Promise<RetrievedChunk[]> {
    const {
      topK = 5,
      minScore = 0.5,
      fileFilter,
      typeFilter,
      expandDependencies = false
    } = options;

    // 1. Embed la query
    const queryEmbedding = await this.embedder.embed(query);

    // 2. Recherche hybride (s√©mantique + keywords)
    let candidates = await this.store.hybridSearch({
      embedding: queryEmbedding,
      text: query,
      limit: topK * 3  // R√©cup√©rer plus pour le reranking
    });

    // 3. Appliquer les filtres
    if (fileFilter) {
      candidates = candidates.filter(c =>
        fileFilter.some(pattern => minimatch(c.filePath, pattern))
      );
    }

    if (typeFilter) {
      candidates = candidates.filter(c => typeFilter.includes(c.type));
    }

    // 4. Reranking avec cross-encoder
    const reranked = await this.reranker.rerank(query, candidates, topK);

    // 5. Filtrer par score minimum
    let results = reranked.filter(c => c.rerankScore >= minScore);

    // 6. Expansion optionnelle des d√©pendances
    if (expandDependencies) {
      results = await this.expandWithDependencies(results);
    }

    return results;
  }

  /**
   * Ajoute les chunks import√©s par les r√©sultats principaux.
   * Permet de fournir plus de contexte au LLM.
   */
  private async expandWithDependencies(
    chunks: RetrievedChunk[]
  ): Promise<RetrievedChunk[]> {
    const expanded = [...chunks];
    const seen = new Set(chunks.map(c => c.id));

    for (const chunk of chunks) {
      // R√©cup√©rer les chunks des fichiers import√©s
      for (const importPath of chunk.imports ?? []) {
        const imported = await this.store.getByFile(importPath);

        for (const dep of imported) {
          if (!seen.has(dep.id)) {
            expanded.push({ ...dep, isExpanded: true });
            seen.add(dep.id);
          }
        }
      }
    }

    return expanded;
  }
}
```

---

## 7.7 üìä √âvaluation du RAG

### 7.7.1 M√©triques cl√©s

Pour savoir si votre RAG fonctionne bien, il faut le mesurer avec des m√©triques standardis√©es :

| M√©trique | Description | Formule | Cible |
|----------|-------------|---------|:-----:|
| **Recall@K** | % de docs pertinents dans top K | pertinents ‚à© topK / pertinents | > 80% |
| **Precision@K** | % de top K qui sont pertinents | pertinents ‚à© topK / K | > 60% |
| **MRR** | Rang moyen du 1er pertinent | 1 / rang_premier_pertinent | > 0.7 |
| **Latence** | Temps de retrieval | ms | < 100ms |

![Metriques RAG](images/rag-metrics.svg)

### 7.7.2 Benchmark maison

Cr√©ez un benchmark sp√©cifique √† votre codebase pour √©valuer votre RAG :

```typescript
// src/context/benchmark.ts

interface RAGBenchmark {
  queries: Array<{
    query: string;
    relevantChunks: string[];  // IDs des chunks pertinents
  }>;
}

interface RAGMetrics {
  recallAtK: number;
  precisionAtK: number;
  mrr: number;
  avgLatencyMs: number;
}

async function evaluateRAG(
  retriever: CodebaseRetriever,
  benchmark: RAGBenchmark,
  k: number = 5
): Promise<RAGMetrics> {
  let totalRecall = 0;
  let totalPrecision = 0;
  let totalMRR = 0;
  let totalLatency = 0;

  for (const { query, relevantChunks } of benchmark.queries) {
    // Mesurer le temps
    const start = Date.now();
    const retrieved = await retriever.retrieve(query, { topK: k });
    totalLatency += Date.now() - start;

    const retrievedIds = new Set(retrieved.map(r => r.id));
    const relevantSet = new Set(relevantChunks);

    // Recall : combien de pertinents trouv√©s
    const foundRelevant = relevantChunks.filter(id => retrievedIds.has(id));
    totalRecall += foundRelevant.length / relevantChunks.length;

    // Precision : combien de trouv√©s sont pertinents
    const relevantFound = retrieved.filter(r => relevantSet.has(r.id));
    totalPrecision += relevantFound.length / k;

    // MRR : 1/rang du premier pertinent
    const firstRelevantRank = retrieved.findIndex(r => relevantSet.has(r.id));
    if (firstRelevantRank >= 0) {
      totalMRR += 1 / (firstRelevantRank + 1);
    }
  }

  const n = benchmark.queries.length;
  return {
    recallAtK: totalRecall / n,
    precisionAtK: totalPrecision / n,
    mrr: totalMRR / n,
    avgLatencyMs: totalLatency / n
  };
}

// Exemple de benchmark
const myBenchmark: RAGBenchmark = {
  queries: [
    {
      query: "Comment fonctionne l'authentification ?",
      relevantChunks: ['auth-service:42', 'auth-middleware:15', 'jwt-utils:8']
    },
    {
      query: "processPayment",
      relevantChunks: ['payment-service:120', 'payment-types:5']
    }
    // ... 20+ queries
  ]
};
```

---

## üìù Points Cl√©s

| Concept | Point cl√© |
|---------|-----------|
| üö´ **Probl√®me** | LLM ne conna√Æt pas votre code, connaissance fig√©e |
| üîÑ **Solution RAG** | Retrieve ‚Üí Augment ‚Üí Generate |
| üßÆ **Embeddings** | Repr√©sentation vectorielle du sens (384-3072 dim) |
| ‚úÇÔ∏è **Chunking** | D√©couper par unit√©s logiques via AST, pas par lignes |
| üîÄ **Hybride** | S√©mantique + keywords = meilleurs r√©sultats |
| üèÜ **Reranking** | Cross-encoder pour affiner le top K |
| üìä **M√©triques** | Recall@K > 80%, Precision@K > 60%, Latence < 100ms |

---

## üèãÔ∏è Exercices

### Exercice 1 : Indexation
**Objectif** : Indexer votre propre projet

```bash
# 1. Mesurez le temps et l'espace disque
time node scripts/index-codebase.js ./my-project

# 2. Notez les statistiques
# - Nombre de fichiers index√©s
# - Nombre de chunks g√©n√©r√©s
# - Taille de la base SQLite
```

### Exercice 2 : Comparaison de chunking
**Objectif** : Comparer chunking par lignes vs par AST

| M√©thode | Recall@5 | Precision@5 | Observations |
|---------|:--------:|:-----------:|--------------|
| Lignes (50) | | | |
| AST | | | |

### Exercice 3 : Tuning hybride
**Objectif** : Trouver le meilleur ratio s√©mantique/keyword

Testez ces configurations sur votre benchmark :

| Ratio S√©mantique/Keyword | Recall@5 | Observations |
|:------------------------:|:--------:|--------------|
| 1.0 / 0.0 | | S√©mantique pur |
| 0.8 / 0.2 | | |
| 0.7 / 0.3 | | D√©faut Grok-CLI |
| 0.5 / 0.5 | | √âquilibr√© |

### Exercice 4 : Cr√©er un benchmark
**Objectif** : Cr√©er 20 queries de test avec leurs chunks pertinents

```typescript
// Cr√©ez votre benchmark
const myBenchmark: RAGBenchmark = {
  queries: [
    // Ajoutez 20 queries repr√©sentatives de votre codebase
  ]
};
```

---

## üìö R√©f√©rences

| Type | R√©f√©rence |
|------|-----------|
| üìÑ Paper | Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" |
| üìÑ Paper | Gao, L., et al. (2023). "Retrieval-Augmented Generation for Large Language Models: A Survey" |
| üíª Code | Grok-CLI : `src/context/codebase-rag/` |
| üìñ Docs | Transformers.js : https://huggingface.co/docs/transformers.js |

---

## üåÖ √âpilogue

*Fin d'apr√®s-midi. Lina teste son nouveau syst√®me RAG.*

**Lina** : "Explique-moi comment fonctionne la fonction `processPayment`."

*Cette fois, l'agent r√©cup√®re le vrai code du projet avant de r√©pondre.*

**Agent** : *"D'apr√®s `src/services/payment-service.ts` lignes 45-78, la fonction `processPayment` prend un objet `Order` et retourne un `PaymentResult`..."*

**Lina** *(souriant)* : "Il conna√Æt vraiment mon code maintenant !"

*Mais son sourire s'efface quand elle lit la suite.*

**Agent** : *"...le type `PaymentResult` est d√©fini dans ce fichier..."*

**Lina** : "Attends. `PaymentResult` n'est PAS d√©fini dans ce fichier. Il est import√© de `types.ts`."

*Elle v√©rifie.*

**Lina** : "Le RAG a r√©cup√©r√© le bon fichier, mais il ne comprend pas les imports. Il ne sait pas que `PaymentResult` vient d'ailleurs."

**Marc** *(arrivant avec son caf√©)* : "C'est le probl√®me classique. Le RAG r√©cup√®re des morceaux pertinents, mais il ne comprend pas les **relations** entre eux."

**Lina** : "Donc si je demande 'modifie le type de retour de processPayment', il ne saura pas o√π aller ?"

**Marc** : "Exactement. Il faut lui donner la conscience du graphe de d√©pendances. Savoir que `payment-service.ts` importe de `types.ts`, qui importe de `common.ts`..."

*Il pose sa tasse.*

**Marc** : "C'est ce qu'on appelle le **Dependency-Aware RAG**. Le RAG nouvelle g√©n√©ration."

**Lina** *(ouvrant son carnet)* : "Montre-moi comment √ßa marche."

---

**√Ä suivre** : *Chapitre 8 ‚Äî Dependency-Aware RAG*

*Le RAG classique trouve les fichiers pertinents. Mais peut-il comprendre qu'un fichier A importe B qui d√©pend de C ? La r√©ponse change tout pour les grandes codebases.*

---

<div align="center">

**‚Üê [Chapitre 6 : Repair et R√©flexion](06-repair-reflexion.md)** | **[Sommaire](README.md)** | **[Chapitre 8 : Dependency-Aware RAG](08-dependency-aware-rag.md) ‚Üí**

</div>
